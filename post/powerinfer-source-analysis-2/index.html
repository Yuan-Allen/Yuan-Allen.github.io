<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>PowerInferæºç è§£æï¼ˆäºŒï¼‰ï¼šè®¡ç®—å›¾æ„å»ºä¸ç®—å­è°ƒç”¨ | AllenY&#39;s blog</title>
<link rel="shortcut icon" href="https://alleny.xyz/favicon.ico?v=1747120786321">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://alleny.xyz/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="PowerInferæºç è§£æï¼ˆäºŒï¼‰ï¼šè®¡ç®—å›¾æ„å»ºä¸ç®—å­è°ƒç”¨ | AllenY&#39;s blog - Atom Feed" href="https://alleny.xyz/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">


<script async src="https://www.googletagmanager.com/gtag/js?id=G-HPLP8D1S43"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-HPLP8D1S43');
</script>


    <meta name="description" content="ä¸Šç¯‡æ–‡ç« æ¦‚æ‹¬äº†PowerInferçš„æ¨¡å‹åŠ è½½è¿‡ç¨‹ï¼Œè¿™æ¬¡æˆ‘ä»¬æ¥çœ‹ä¸€çœ‹æ¨ç†æ—¶çš„æµç¨‹ã€‚

ç›¸å…³

PowerInferæºç è§£æï¼ˆä¸€ï¼‰ï¼šæ¨¡å‹åŠ è½½ã€‚
PowerInferæºç è§£æï¼ˆä¸‰ï¼‰ï¼šç®—å­å®ç°ã€‚

è®¡ç®—å›¾æ„å»º
è®©æˆ‘ä»¬å›åˆ°examples/main/..." />
    <meta name="keywords" content="System for AI,Technical Sharing" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.5.1/build/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://alleny.xyz">
  <img class="avatar" src="https://alleny.xyz/images/avatar.png?v=1747120786321" alt="">
  </a>
  <h1 class="site-title">
    AllenY&#39;s blog
  </h1>
  <p class="site-description">
    ğŸ§‘ğŸ»â€ğŸ’»ğŸ®ğŸ¿ğŸ¹ğŸ—»
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          Home
        </a>
      
    
      
        <a href="/archives" class="menu">
          Archive
        </a>
      
    
      
        <a href="/tags" class="menu">
          Tags
        </a>
      
    
      
        <a href="/post/about" class="menu">
          About
        </a>
      
    
  </div>
  <div class="social-container">
    
      
        <a href="https://github.com/Yuan-Allen" target="_blank">
          <i class="ri-github-line"></i>
        </a>
      
    
      
    
      
        <a href="https://weibo.com/u/6478080851" target="_blank">
          <i class="ri-weibo-line"></i>
        </a>
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              PowerInferæºç è§£æï¼ˆäºŒï¼‰ï¼šè®¡ç®—å›¾æ„å»ºä¸ç®—å­è°ƒç”¨
            </h2>
            <div class="post-info">
              <span>
                2024-09-25
              </span>
              <span>
                25 min read
              </span>
              
                <a href="https://alleny.xyz/tag/system-for-ai/" class="post-tag">
                  # System for AI
                </a>
              
                <a href="https://alleny.xyz/tag/WT-UATSMl/" class="post-tag">
                  # Technical Sharing
                </a>
              
            </div>
            
              <img class="post-feature-image" src="https://alleny.xyz/post-images/powerinfer-source-analysis-2.png" alt="">
            
            <div class="post-content-wrapper">
              <div class="post-content" v-pre>
                <p>ä¸Šç¯‡æ–‡ç« æ¦‚æ‹¬äº†PowerInferçš„æ¨¡å‹åŠ è½½è¿‡ç¨‹ï¼Œè¿™æ¬¡æˆ‘ä»¬æ¥çœ‹ä¸€çœ‹æ¨ç†æ—¶çš„æµç¨‹ã€‚</p>
<!-- more -->
<h2 id="ç›¸å…³">ç›¸å…³</h2>
<ul>
<li><a href="https://alleny.xyz/post/powerinfer-source-analysis-1/">PowerInferæºç è§£æï¼ˆä¸€ï¼‰ï¼šæ¨¡å‹åŠ è½½</a>ã€‚</li>
<li><a href="https://alleny.xyz/post/powerinfer-source-analysis-3/">PowerInferæºç è§£æï¼ˆä¸‰ï¼‰ï¼šç®—å­å®ç°</a>ã€‚</li>
</ul>
<h2 id="è®¡ç®—å›¾æ„å»º">è®¡ç®—å›¾æ„å»º</h2>
<p>è®©æˆ‘ä»¬å›åˆ°<code>examples/main/main.cpp:main</code>ï¼Œæ¨¡å‹åœ¨åŠ è½½ä¹‹åï¼Œåˆç»è¿‡äº†ä¸€å †æ¨ç†æ—¶çš„å‚æ•°å‡†å¤‡ï¼Œæœ€ç»ˆè°ƒç”¨<code>llama_decode</code>è¿›è¡Œæ¨ç†ã€‚</p>
<pre><code class="language-c++">int main(int argc, char ** argv) {
    // -- snip --
                for (int i = 0; i &lt; (int) embd.size(); i += params.n_batch) {
                int n_eval = (int) embd.size() - i;
                if (n_eval &gt; params.n_batch) {
                    n_eval = params.n_batch;
                }

                LOG(&quot;eval: %s\n&quot;, LOG_TOKENS_TOSTR_PRETTY(ctx, embd).c_str());

                if (llama_decode(ctx, llama_batch_get_one(&amp;embd[i], n_eval, n_past, 0))) {
                    LOG_TEE(&quot;%s : failed to eval\n&quot;, __func__);
                    return 1;
                }

                n_past += n_eval;

                LOG(&quot;n_past = %d\n&quot;, n_past);
            }
    // -- snip --
}
</code></pre>
<p><code>llama_decode</code>ä¼šè°ƒç”¨åˆ°<code>llama_decode_internal</code>ï¼Œ<code>llama_decode_internal</code>åˆ™ä¼šä½¿ç”¨<code>llama_build_graph</code>æ¥æ„å»ºè®¡ç®—å›¾ã€‚<br>
å¯¹äº<code>LLM_ARCH_LLAMA</code>å’Œ<code>LLM_ARCH_BAMBOO</code>æ¨¡å‹æ¶æ„ï¼ŒPowerInferè°ƒç”¨<code>llm.build_llama_variants</code>æ¥ç”Ÿæˆè®¡ç®—å›¾ï¼ˆæ›¿ä»£äº†åŸæœ‰çš„<code>llm.build_llama</code>å‡½æ•°ï¼‰ã€‚<br>
å¯¹åŸå…ˆçš„<code>build_llama</code>è€Œè¨€ï¼Œæ–°çš„<code>build_llama_variants</code>çš„åŒºåˆ«ä¸»è¦åœ¨äºï¼Œå¯¹äºä½¿ç”¨sparse inferenceçš„æƒ…å†µï¼ŒPowerInferä¼šä½¿ç”¨å•ç‹¬çš„<code>llm_build_ffn_sparse</code>ï¼ˆåŒºåˆ«äº<code>llm_build_ffn</code>ï¼‰æ¥ç”ŸæˆFFNéƒ¨åˆ†ã€‚</p>
<pre><code class="language-c++">    struct ggml_cgraph * build_llama_variants() {
        // -- snip --
                if (llama_use_sparse_inference(&amp;model)) {
                    llm_build_cb_short cbs = [&amp;](ggml_tensor * cur, const char * name) {
                        std::string name_str = std::string(name) + &quot;-&quot; + std::to_string(il);
                        ggml_set_name(cur, name_str.c_str());
                    };
                    // We only offload the ffn input to GPU if all neurons are offloaded
                    if (model.layers[il].gpu_offload_ratio &gt;= 1.) {
                        cb(cur, &quot;ffn_norm&quot;, il);
                    } else {
                        cbs(cur, &quot;ffn_norm&quot;);
                    }
                    cur = llm_build_ffn_sparse(ctx0, cur,
                        model.layers[il].ffn_up,   NULL,
                        model.layers[il].ffn_gate, NULL,
                        model.layers[il].ffn_down_t, NULL,
                        model.layers[il].mlp_pre_w1,
                        model.layers[il].mlp_pre_w2,
                        ffn_inp, // as for now, llama's pred use the same input as the ffn
                        model.layers[il].gpu_idx, 
                        model.layers[il].gpu_bucket, model.layers[il].ffn_gate_gpu, model.layers[il].ffn_down_gpu, model.layers[il].ffn_up_gpu,
                        LLM_FFN_RELU, gate_type, model.layers[il].gpu_offload_ratio, cbs);
                } else {
                    // fallback to dense
                    cb(cur, &quot;ffn_norm&quot;, il);
                    llm_ffn_op_type   act_type = model.arch == LLM_ARCH_BAMBOO ? LLM_FFN_RELU : LLM_FFN_SILU;
                    cur = llm_build_ffn(ctx0, cur,
                        model.layers[il].ffn_up,   NULL,
                        model.layers[il].ffn_gate, NULL,
                        model.layers[il].ffn_down, NULL,
                        act_type, gate_type, cb, il);
                }
    // -- snip --
}
</code></pre>
<p>åœ¨<code>llm_build_ffn_sparse</code>ä¸­æˆ‘ä»¬å¯ä»¥å…·ä½“çœ‹è§FFNéƒ¨åˆ†æ˜¯å¦‚ä½•è¢«æ„å»ºçš„ã€‚æˆ‘ä»¬å¯ä»¥å…ˆçœ‹ä¸€ä¸‹åŸç‰ˆLlamaçš„MLPçš„å…·ä½“æ¶æ„ã€‚<br>
<img src="https://alleny.xyz/post-images/1727258017810.png" alt="" loading="lazy"><br>
å›¾ä¸­çš„Gateï¼ŒUpå’ŒDownå‡ä¸ºçº¿æ€§å±‚ã€‚å³Llamaçš„MLPçš„è¾“å…¥ä¼šä¸€è¾¹ç»è¿‡Gateç„¶åæ¿€æ´»å‡½æ•°ï¼Œå¦ä¸€è¾¹ç»è¿‡Upï¼Œæœ€åä¸¤è€…ä¹˜èµ·æ¥å†ç»è¿‡Downï¼Œæœ€ç»ˆå¾—åˆ°MLPçš„è¾“å‡ºã€‚<br>
è‡³äºPowerInferï¼Œåˆ™åœ¨å‰é¢åŠ äº†ä¸€ä¸ªPredictorç”¨æ¥é¢„æµ‹å½“å‰å±‚çš„å“ªäº›neuronsä¼šè¢«æ¿€æ´»ã€‚å¹¶ä¸”ï¼Œè¿™äº›neuronså¯èƒ½æ˜¯åˆ†åˆ«åˆ†å¸ƒåœ¨GPUå’ŒCPUä¸Šçš„ï¼ŒåŒæ—¶è¿˜è¦è€ƒè™‘åˆ©ç”¨åˆ°æ¿€æ´»çš„ç¨€ç–æ€§ï¼Œå› æ­¤è¿˜ä¼šè°ƒç”¨ç‰¹åˆ¶çš„ç®—å­æ¥è¿›è¡Œè®¡ç®—ã€‚<br>
<img src="https://alleny.xyz/post-images/1727323918446.png" alt="" loading="lazy"><br>
äºæ˜¯æ¥çœ‹çœ‹ä»£ç ã€‚</p>
<pre><code class="language-c++">static struct ggml_tensor * llm_build_ffn_sparse(
        struct ggml_context * ctx,
         struct ggml_tensor * cur,
         struct ggml_tensor * up,
         struct ggml_tensor * up_b,
         struct ggml_tensor * gate,
         struct ggml_tensor * gate_b,
         struct ggml_tensor * down_t,
         struct ggml_tensor * down_b,
         struct ggml_tensor * pre_w1,
         struct ggml_tensor * pre_w2,
         struct ggml_tensor * pred_inpl,
         struct ggml_tensor * gpu_index,
         struct ggml_tensor * gpu_bucket,
         struct ggml_tensor * gate_gpu,
         struct ggml_tensor * down_gpu,
         struct ggml_tensor * up_gpu,
            llm_ffn_op_type   type_op,
          llm_ffn_gate_type   type_gate,
                     double   gpu_offload_ratio,
   const llm_build_cb_short &amp; cb_outer) {
    bool full_gpu = gpu_offload_ratio &gt;= 1.0;
    ggml_tensor * ffn_input = cur;

    llm_build_cb_short cb = [&amp;cb_outer](struct ggml_tensor * tensor, const char * name) {
        cb_outer(tensor, name);
#if defined(GGML_USE_CUBLAS)
        // Determine offloading based on src[0] (weight for both mul and axpy)
        bool operates_on_gpu = tensor-&gt;src[0]-&gt;backend == GGML_BACKEND_GPU;
        if (operates_on_gpu) {
            ggml_cuda_assign_buffers_no_alloc(tensor);
        }
#endif
    };

    // prepare sparse idx
    ggml_tensor * idx = ggml_mul_mat(ctx, pre_w1, pred_inpl);
    cb(idx, &quot;mlp_pre_hidden&quot;);
    idx = ggml_relu(ctx, idx);
    cb(idx, &quot;mlp_pre_relu&quot;);
    idx = ggml_mul_mat(ctx, pre_w2, idx);
    // If the FFN layer is not fully offloaded, we need to transfer the sparsity index
    // back to the CPU to avoid synchronization issues.
    (full_gpu ? cb : cb_outer)(idx, &quot;mlp_pre_out&quot;);

    auto act_fn = [&amp;](ggml_tensor * tensor, const char * name) {
        switch (type_op) {
            case LLM_FFN_RELU:
                {
                    tensor = ggml_relu(ctx, tensor);
                    cb(tensor, name);
                } break;
            default:
                GGML_ASSERT(false &amp;&amp; &quot;unsupported activation function&quot;);
        }
        return tensor;
    };

    // FFN up
    struct ggml_tensor * up_out = llm_build_sparse_mul_mat(ctx, up, ffn_input, idx, up_gpu, gpu_index, gpu_bucket, cb_outer, &quot;up&quot;, full_gpu);
    if (up_b) {
        up_out = ggml_add(ctx, up_out, up_b);
        cb(up_out, &quot;ffn_up_b&quot;);
    }

    struct ggml_tensor * gate_out = nullptr;
    if (gate) {
        ggml_tensor * gate_input = (type_gate == LLM_FFN_PAR || type_gate == LLM_FFN_SYM) ? ffn_input : up_out;
        gate_out = llm_build_sparse_mul_mat(ctx, gate, gate_input, idx, gate_gpu, gpu_index, gpu_bucket, cb_outer, &quot;gate&quot;, full_gpu);
        if (gate_b) {
            gate_out = ggml_add(ctx, gate_out, gate_b);
            cb(gate_out, &quot;ffn_gate_b&quot;);
        }
        switch (type_gate) {
            case LLM_FFN_PAR:
                {
                    ggml_tensor * act_gate = act_fn(gate_out, &quot;ffn_gate_act&quot;);
                    cur = ggml_mul(ctx, act_gate, up_out);
                    cb(cur, &quot;ffn_gate_par&quot;);
                } break;
            case LLM_FFN_SYM:
                {
                    ggml_tensor * act_gate = act_fn(gate_out, &quot;ffn_gate_act&quot;);
                    ggml_tensor * act_up = act_fn(up_out, &quot;ffn_up_act&quot;);
                    cur = ggml_mul(ctx, act_gate, act_up);
                    cb(cur, &quot;ffn_gate_sym&quot;);
                } break;
            case LLM_FFN_SEQ:
                {
                    cur = act_fn(gate_out, &quot;ffn_gate_act&quot;);
                } break;
            default: GGML_ASSERT(false &amp;&amp; &quot;unsupported gate type&quot;);
        }
    } else {
        cur = act_fn(up_out, &quot;ffn_up_act&quot;);
    }

    cur = llm_build_sparse_axpy(ctx, down_t, cur, idx, down_gpu, gpu_index, gpu_bucket, cb_outer, &quot;down&quot;, full_gpu);

    if (down_b) {
        cur = ggml_add(ctx, cur, down_b);
        cb(cur, &quot;ffn_down_b&quot;);
    }

    return cur;
}
</code></pre>
<p>è®©æˆ‘ä»¬æ¥å…·ä½“çœ‹ä¸€ä¸‹é‡Œé¢çš„å†…å®¹ã€‚<br>
é¦–å…ˆæ˜¯ä¸€ä¸ªçŸ©é˜µä¹˜ã€ä¸€ä¸ªReLUã€å†ä¸€ä¸ªçŸ©é˜µä¹˜ï¼Œç»„æˆäº†predictoréƒ¨åˆ†ï¼Œå¼ é‡åå­—åˆ†åˆ«ä¸º<code>mlp_pre_hidden</code>ã€<code>mlp_pre_relu</code>å’Œ<code>mlp_pre_out</code>ï¼Œæœ€ç»ˆå¾—åˆ°sparse idxï¼Œå³å“ªäº›neuronsä¼šè¢«æ¿€æ´»çš„é¢„æµ‹ç»“æœã€‚</p>
<pre><code class="language-c++">    // prepare sparse idx
    ggml_tensor * idx = ggml_mul_mat(ctx, pre_w1, pred_inpl);
    cb(idx, &quot;mlp_pre_hidden&quot;);
    idx = ggml_relu(ctx, idx);
    cb(idx, &quot;mlp_pre_relu&quot;);
    idx = ggml_mul_mat(ctx, pre_w2, idx);
    // If the FFN layer is not fully offloaded, we need to transfer the sparsity index
    // back to the CPU to avoid synchronization issues.
    (full_gpu ? cb : cb_outer)(idx, &quot;mlp_pre_out&quot;);
</code></pre>
<p>å®šä¹‰äº†åç»­æ‰€ä½¿ç”¨çš„æ¿€æ´»å‡½æ•°ï¼Œç›®å‰åªæ”¯æŒReLUã€‚</p>
<pre><code class="language-c++">    auto act_fn = [&amp;](ggml_tensor * tensor, const char * name) {
        switch (type_op) {
            case LLM_FFN_RELU:
                {
                    tensor = ggml_relu(ctx, tensor);
                    cb(tensor, name);
                } break;
            default:
                GGML_ASSERT(false &amp;&amp; &quot;unsupported activation function&quot;);
        }
        return tensor;
    };
</code></pre>
<p>å›å¿†Llama MLPçš„ç»“æ„ï¼Œé¦–å…ˆä¸€è¾¹æ˜¯<code>ffn_up</code>ã€‚</p>
<pre><code class="language-c++">    // FFN up
    struct ggml_tensor * up_out = llm_build_sparse_mul_mat(ctx, up, ffn_input, idx, up_gpu, gpu_index, gpu_bucket, cb_outer, &quot;up&quot;, full_gpu);
    if (up_b) {
        up_out = ggml_add(ctx, up_out, up_b);
        cb(up_out, &quot;ffn_up_b&quot;);
    }
</code></pre>
<p>å¦ä¸€è¾¹æ˜¯<code>ffn_gate</code>ä»¥åŠåç»­çš„æ¿€æ´»å‡½æ•°ã€‚<br>
æ³¨æ„è¿™é‡Œåˆæœ‰ä¸€ä¸ªgateç±»å‹çš„é€‰æ‹©ã€‚å…¶ç±»å‹åˆ†åˆ«æœ‰ï¼š</p>
<ul>
<li><code>LLM_FFN_SEQ</code>ï¼š<code>ffn_gate</code>å’Œ<code>ffn_up</code>çš„è®¡ç®—æ˜¯é¡ºåºçš„ï¼Œå…ˆè®¡ç®—<code>ffn_up</code>ï¼Œå†æŠŠ<code>ffn_up</code>çš„ç»“æœä½œä¸º<code>ffn_gate</code>çš„è¾“å…¥ã€‚</li>
<li><code>LLM_FFN_PAR</code>ï¼š<code>ffn_gate</code>ä¸<code>ffn_up</code>çš„è®¡ç®—æ˜¯å¹¶è¡Œçš„ã€‚<code>ffn_gate</code>çš„è¾“å…¥æ˜¯<code>ffn_input</code>ï¼Œç­‰è¾“å…¥ç»è¿‡<code>ffn_gate</code>å’Œæ¿€æ´»å‡½æ•°åå†å’Œ<code>up_out</code>ç›¸ä¹˜ã€‚</li>
<li><code>LLM_FFN_SYM</code>ï¼šä¸<code>LLM_FFN_PAR</code>çš„åŒºåˆ«æ˜¯ï¼Œ<code>up_out</code>ä¹Ÿè¦å…ˆç»è¿‡æ¿€æ´»å‡½æ•°æ‰ä¸<code>ffn_gate_act</code>ç›¸ä¹˜ã€‚<br>
æœ€åï¼Œç»“æœä¼šå†ç»è¿‡<code>ffn_down</code>ï¼Œè¿›è¡Œä¸€ä¸ªaxpyæ“ä½œï¼Œå¾—åˆ°æœ€ç»ˆç»“æœã€‚</li>
</ul>
<pre><code class="language-c++">    struct ggml_tensor * gate_out = nullptr;
    if (gate) {
        ggml_tensor * gate_input = (type_gate == LLM_FFN_PAR || type_gate == LLM_FFN_SYM) ? ffn_input : up_out;
        gate_out = llm_build_sparse_mul_mat(ctx, gate, gate_input, idx, gate_gpu, gpu_index, gpu_bucket, cb_outer, &quot;gate&quot;, full_gpu);
        if (gate_b) {
            gate_out = ggml_add(ctx, gate_out, gate_b);
            cb(gate_out, &quot;ffn_gate_b&quot;);
        }
        switch (type_gate) {
            case LLM_FFN_PAR:
                {
                    ggml_tensor * act_gate = act_fn(gate_out, &quot;ffn_gate_act&quot;);
                    cur = ggml_mul(ctx, act_gate, up_out);
                    cb(cur, &quot;ffn_gate_par&quot;);
                } break;
            case LLM_FFN_SYM:
                {
                    ggml_tensor * act_gate = act_fn(gate_out, &quot;ffn_gate_act&quot;);
                    ggml_tensor * act_up = act_fn(up_out, &quot;ffn_up_act&quot;);
                    cur = ggml_mul(ctx, act_gate, act_up);
                    cb(cur, &quot;ffn_gate_sym&quot;);
                } break;
            case LLM_FFN_SEQ:
                {
                    cur = act_fn(gate_out, &quot;ffn_gate_act&quot;);
                } break;
            default: GGML_ASSERT(false &amp;&amp; &quot;unsupported gate type&quot;);
        }
    } else {
        cur = act_fn(up_out, &quot;ffn_up_act&quot;);
    }
</code></pre>
<p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ<code>llm_build_ffn_sparse</code>è°ƒç”¨äº†ä¸“é—¨çš„<code>llm_build_sparse_mul_mat</code>æ¥è¿›è¡ŒçŸ©é˜µä¹˜ï¼Œå…¶ä»¥ä½¿ç”¨sparse idxæ¥åˆ©ç”¨æ¿€æ´»çš„ç¨€ç–æ€§ç‰¹å¾ã€‚<br>
åœ¨<code>llm_build_sparse_mul_mat</code>ä¸­ï¼Œå¦‚æœæ˜¯å…¨éƒ¨è®¡ç®—å¸è½½åˆ°GPUçš„æƒ…å†µåˆ™ç›´æ¥å¯¹<code>up_gpu</code>å’Œè¾“å…¥è°ƒç”¨<code>ggml_mul_mat_idx</code>å¹¶è¿”å›å³å¯ã€‚<br>
å¦åˆ™ï¼Œå¯¹CPUä¸Šçš„<code>up</code>å’ŒGPUä¸Šçš„<code>up_gpu</code>åˆ†åˆ«ä½¿ç”¨<code>ggml_mul_mat_idx</code>å’Œ<code>ggml_mul_mat_idx_upscale</code>è¿›è¡Œè®¡ç®—ï¼Œç„¶åå†æŠŠå¾—åˆ°çš„ç»“æœ<code>out</code>å’Œ<code>out_gpu</code>ä½¿ç”¨<code>ggml_add</code>è¿›è¡Œæ±‡æ€»ï¼Œå¾—åˆ°æœ€ç»ˆç»“æœã€‚</p>
<pre><code class="language-c++">static struct ggml_tensor * llm_build_sparse_mul_mat(
        struct ggml_context * ctx,
         struct ggml_tensor * up,
         struct ggml_tensor * inp,
         struct ggml_tensor * idx,
         struct ggml_tensor * up_gpu,
         struct ggml_tensor * gpu_index,
         struct ggml_tensor * gpu_bucket,
   const llm_build_cb_short &amp; cb,
                 const char * name,
                         bool full_gpu) {
    std::string full_name = &quot;ffn_&quot; + std::string(name) + &quot;_sparse&quot;;
    ggml_tensor * out = nullptr;

#ifdef GGML_USE_HIPBLAS
// WARNING: THIS IS A HACK! 
// if up_gpu-&gt;data is null
// inference fails when model exceeds 40B on rocm device
// so we just let up_gpu-&gt;data point to itself
    
    up_gpu-&gt;data = up_gpu;

#endif 

#ifdef GGML_USE_CUBLAS
    // Full offloading fast path
    if (full_gpu) {
        GGML_ASSERT(up_gpu &amp;&amp; &quot;full_gpu but no up_gpu&quot;);
        out = ggml_mul_mat_idx(ctx, up_gpu, inp, idx, NULL);
        ggml_cuda_assign_buffers_no_alloc(out);
        cb(out, (full_name).c_str());
        return out;
    }
#endif

    out = ggml_mul_mat_idx(ctx, up, inp, idx, gpu_index);
    cb(out, full_name.c_str());

#ifdef GGML_USE_CUBLAS
    if (up_gpu) {
        ggml_tensor * out_gpu = ggml_mul_mat_idx_upscale(ctx, up_gpu, inp, idx, gpu_bucket, out-&gt;ne[0]);
        ggml_cuda_assign_buffers_no_alloc(out_gpu);
        cb(out_gpu, (full_name + &quot;_gpu&quot;).c_str());
        out = ggml_add(ctx, out, out_gpu);
        // We don't need to assign buffers here, as the output will be passed into Axpy,
        // which in this case, is also a hybrid operation.
        cb(out, (full_name + &quot;_merged&quot;).c_str());
    }
#endif

    return out;
}
</code></pre>
<p>æŸ¥çœ‹æ‰€è°ƒç”¨çš„<code>ggml_mul_mat_idx</code>ã€‚æˆ‘ä»¬å¯ä»¥çœ‹è§ï¼Œ<code>a</code>ï¼Œ<code>b</code>ï¼Œ<code>sparse idx</code>ï¼ˆç”±predictorå¾—åˆ°ï¼‰å’Œ<code>gpu_idx</code>ï¼ˆofflineæ—¶å€™å¾—åˆ°ï¼‰åˆ†åˆ«è¢«ä½œä¸ºäº†å››ä¸ª<code>src</code>å‚ä¸è®¡ç®—ã€‚<code>op</code>ä½¿ç”¨çš„æ˜¯<code>GGML_OP_MUL_MAT_SPARSE</code>ï¼ˆåŒºåˆ«äº<code>GGML_OP_MUL_MAT</code>ï¼‰ã€‚</p>
<pre><code class="language-c++">struct ggml_tensor * ggml_mul_mat_idx(
        struct ggml_context * ctx,
        struct ggml_tensor  * a,
        struct ggml_tensor  * b,
        struct ggml_tensor  * sparse_idx,
        // Under hybrid inference, this tensor is to indicate which row are offloaded to GPU;
        // When using full GPU inference, it is NULL.
        struct ggml_tensor  * gpu_idx) {
    GGML_ASSERT(!ggml_is_transposed(a));

    bool is_node = false;

    if (a-&gt;grad || b-&gt;grad) {
        is_node = true;
    }

    const int64_t ne[4] = { a-&gt;ne[1], b-&gt;ne[1], b-&gt;ne[2], b-&gt;ne[3] };
    struct ggml_tensor * result = ggml_new_tensor(ctx, GGML_TYPE_F32, MAX(a-&gt;n_dims, b-&gt;n_dims), ne);

    result-&gt;op   = GGML_OP_MUL_MAT_SPARSE;
    result-&gt;grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;
    result-&gt;src[0] = a;
    result-&gt;src[1] = b;
    result-&gt;src[2] = sparse_idx;
    result-&gt;src[3] = gpu_idx;

    int32_t params[] = { gpu_idx ? 0 : 1 };
    ggml_set_op_params(result, params, sizeof(params));

    return result;
}
</code></pre>
<h2 id="ç®—å­è°ƒç”¨">ç®—å­è°ƒç”¨</h2>
<p>å›åˆ°<code>llama_decode_internal</code>ä¸­ã€‚è®¡ç®—å›¾è¢«æ„å»ºä»¥åï¼Œåç»­åˆ™ä¼šè¢«ä½¿ç”¨<code>ggml_graph_compute_helper</code>è¿›è¡Œè®¡ç®—ã€‚<br>
<code>ggml_graph_compute_helper</code>ä¼šè°ƒç”¨<code>ggml_graph_plan</code>è¯„ä¼°å·¥ä½œç¼“å†²åŒºçš„å¤§å°ï¼Œå¾—åˆ°ä¸€ä¸ª<code>ggml_cplan</code>ï¼ˆä¸»è¦æˆå‘˜ä¸º<code>work_size</code>ï¼Œ<code>work_data</code>å’Œ<code>n_threads</code>ï¼‰ã€‚ç„¶åï¼Œä¼šä½¿ç”¨<code>ggml_graph_compute(graph, &amp;plan)</code>è¿›è¡Œè®¡ç®—ã€‚</p>
<pre><code class="language-c++">static void ggml_graph_compute_helper(std::vector&lt;uint8_t&gt; &amp; buf, ggml_cgraph * graph, int n_threads) {
    struct ggml_cplan plan = ggml_graph_plan(graph, n_threads);

    if (plan.work_size &gt; 0) {
        buf.resize(plan.work_size);
        plan.work_data = buf.data();
    }

    ggml_graph_compute(graph, &amp;plan);
}
</code></pre>
<p>åŒºåˆ«äºåŸå…ˆçš„ä»£ç ï¼ŒPowerInferæ‰€ä½¿ç”¨çš„<code>state_shared</code>ä¸­<code>n_threads</code>ä¸<code>n_active</code>å‡æ¯”<code>cplan</code>ä¸­çš„å€¼å°‘1ï¼Œè¿™åº”è¯¥æ˜¯ç”±äºPowerInferä¸­æœ‰ä¸€ä¸ªå•ç‹¬çš„çº¿ç¨‹ä¸“é—¨ç”¨ä½œGPU Executorã€‚<br>
ç„¶åï¼ŒPowerInferåˆ›å»ºçš„å·¥ä½œçº¿ç¨‹æ‰€æ‰§è¡Œå‡½æ•°ä¸º<code>ggml_graph_compute_thread_hybrid</code>ï¼ˆåŒºåˆ«äº<code>ggml_graph_compute_thread</code>ï¼‰ï¼Œæœ€ç»ˆå®Œæˆè®¡ç®—ã€‚</p>
<pre><code class="language-c++">int ggml_graph_compute(struct ggml_cgraph * cgraph, struct ggml_cplan * cplan) {
    {
        GGML_ASSERT(cplan);
        GGML_ASSERT(cplan-&gt;n_threads &gt; 0);

        if (cplan-&gt;work_size &gt; 0) {
            GGML_ASSERT(cplan-&gt;work_data);
        }
    }

    const int n_threads = cplan-&gt;n_threads;
#ifdef GGML_USE_HYBRID_THREADING
    struct ggml_compute_state_shared state_shared = {
        /*.cgraph                  =*/ cgraph,
        /*.cgraph_plan             =*/ cplan,
        /*.perf_node_start_cycles  =*/ 0,
        /*.perf_node_start_time_us =*/ 0,
        /*.n_threads               =*/ n_threads-1,
        /*.aic                     =*/ 0,
        /*.n_active                =*/ n_threads-1,
        /*.node_n                  =*/ -1,
        /*.abort_callback          =*/ NULL,
        /*.abort_callback_data     =*/ NULL,
    };
#else
    struct ggml_compute_state_shared state_shared = {
        /*.cgraph                  =*/ cgraph,
        /*.cgraph_plan             =*/ cplan,
        /*.perf_node_start_cycles  =*/ 0,
        /*.perf_node_start_time_us =*/ 0,
        /*.n_threads               =*/ n_threads,
        /*.aic                     =*/ 0,
        /*.n_active                =*/ n_threads,
        /*.node_n                  =*/ -1,
        /*.abort_callback          =*/ NULL,
        /*.abort_callback_data     =*/ NULL,
    };
#endif
    struct ggml_compute_state * workers = alloca(sizeof(struct ggml_compute_state)*n_threads);

    // create thread pool
    if (n_threads &gt; 1) {
        for (int j = 1; j &lt; n_threads; ++j) {
            workers[j] = (struct ggml_compute_state) {
                .thrd   = 0,
                .ith = j,
                .shared = &amp;state_shared,
            };
#ifdef GGML_USE_HYBRID_THREADING
            const int rc = ggml_thread_create(&amp;workers[j].thrd, NULL, ggml_graph_compute_thread_hybrid, &amp;workers[j]);
#else
            const int rc = ggml_thread_create(&amp;workers[j].thrd, NULL, ggml_graph_compute_thread, &amp;workers[j]);
#endif
            GGML_ASSERT(rc == 0);
            UNUSED(rc);
        }
    }

    workers[0].ith = 0;
    workers[0].shared = &amp;state_shared;

    const int64_t perf_start_cycles  = ggml_perf_cycles();
    const int64_t perf_start_time_us = ggml_perf_time_us();

    // this is a work thread too

#ifdef GGML_USE_HYBRID_THREADING
    int compute_status = (size_t) ggml_graph_compute_thread_hybrid(&amp;workers[0]);
#else
    int compute_status = (size_t) ggml_graph_compute_thread(&amp;workers[0]);
#endif

    // don't leave affinity set on the main thread
    clear_numa_thread_affinity();

    // join or kill thread pool
    if (n_threads &gt; 1) {
        for (int j = 1; j &lt; n_threads; j++) {
            const int rc = ggml_thread_join(workers[j].thrd, NULL);
            GGML_ASSERT(rc == 0);
        }
    }

    // performance stats (graph)
    {
        int64_t perf_cycles_cur  = ggml_perf_cycles()  - perf_start_cycles;
        int64_t perf_time_us_cur = ggml_perf_time_us() - perf_start_time_us;

        cgraph-&gt;perf_runs++;
        cgraph-&gt;perf_cycles  += perf_cycles_cur;
        cgraph-&gt;perf_time_us += perf_time_us_cur;

        GGML_PRINT_DEBUG(&quot;%s: perf (%d) - cpu = %.3f / %.3f ms, wall = %.3f / %.3f ms\n&quot;,
                __func__, cgraph-&gt;perf_runs,
                (double) perf_cycles_cur      / (double) ggml_cycles_per_ms(),
                (double) cgraph-&gt;perf_cycles  / (double) ggml_cycles_per_ms() / (double) cgraph-&gt;perf_runs,
                (double) perf_time_us_cur     / 1000.0,
                (double) cgraph-&gt;perf_time_us / 1000.0 / cgraph-&gt;perf_runs);
    }

    return compute_status;
}
</code></pre>
<p>å› æ­¤é‡å¤´æˆå°±åœ¨å·¥ä½œçº¿ç¨‹æ‰€æ‰§è¡Œçš„<code>ggml_graph_compute_thread_hybrid</code>é‡Œé¢ã€‚è¿™æ˜¯ä¸€ä¸ªæ¯”è¾ƒé•¿çš„å‡½æ•°ï¼Œæˆ‘ä»¬æ¥æ¢³ç†ä¸€ä¸‹é‡Œé¢çš„æµç¨‹ã€‚</p>
<ul>
<li>ä¸»è¦çš„å‡½æ•°ä½“æ˜¯ä¸€ä¸ª<code>while(true)</code>çš„å¾ªç¯ï¼Œè®¡ç®—å›¾çš„èŠ‚ç‚¹ä¼šåœ¨é‡Œé¢ä¾æ¬¡å¾—åˆ°è®¡ç®—ã€‚</li>
<li>ç¬¬0å·çº¿ç¨‹(<code>state-&gt;ith == 0</code>)ä¼šè¿›å…¥ä¸€æ¡å•ç‹¬çš„è·¯å¾„ã€‚äº‹å®ä¸Šä»<code>if (node-&gt;backend == GGML_BACKEND_CPU) continue;</code>å¯ä»¥çœ‹å‡ºè¿™æ˜¯ä¸“é—¨ç”¨æ¥è´Ÿè´£GPUè®¡ç®—çš„çº¿ç¨‹ã€‚å…¶ä¼šä½¿ç”¨ä¸€ä¸ª<code>while (1)</code>å¾ªç¯æ¥ç­‰å¾…<code>node-&gt;src[0]-&gt;is_finish</code>ï¼Œ<code>node-&gt;src[1]-&gt;is_finish</code>å’Œ<code>node-&gt;src[2]-&gt;is_finish</code>å‡å‡†å¤‡å°±ç»ªï¼Œç„¶åè®¾ç½®å‚æ•°(<code>params.type = GGML_TASK_COMPUTE</code>)è°ƒç”¨<code>ggml_compute_forward</code>è¿›è¡Œè®¡ç®—ï¼Œä¹‹åå†æŠŠ<code>node-&gt;is_finish</code>ç½®ä¸º1ã€‚ä»¥æ­¤å¾€å¤ï¼Œç›´åˆ°<code>node_n &gt;= cgraph-&gt;n_nodes</code>ã€‚</li>
<li>å…¶ä»–çº¿ç¨‹ï¼ˆé0å·ï¼‰æ˜¯è´Ÿè´£æ‰§è¡ŒCPUè®¡ç®—çš„çº¿ç¨‹ã€‚
<ul>
<li>ç®—å­å¯èƒ½æœ‰<code>INIT</code>å­ä»»åŠ¡å’Œ<code>FINALIZE</code>å­ä»»åŠ¡ï¼Œå¯ä½¿ç”¨<code>GGML_OP_HAS_INIT[node-&gt;op]</code>å’Œ<code>GGML_OP_HAS_FINALIZE[node-&gt;op]</code>æ¥åˆ¤æ–­ã€‚</li>
<li>æ¯ä¸ªçº¿ç¨‹æ¯æ¬¡å¾ªç¯ä¼šå…ˆæ‰§è¡Œä¸€é<code>atomic_fetch_sub(&amp;state-&gt;shared-&gt;n_active, 1)</code>ã€‚
<ul>
<li>æœ€åä¸€ä¸ªæ‰§è¡Œåˆ°è¿™é‡Œï¼ˆè¿”å›å€¼ä¸º1ï¼‰çš„çº¿ç¨‹å°†ä¼šè´Ÿè´£ä¸Šä¸€ä¸ªèŠ‚ç‚¹çš„<code>FINALIZE</code>å­ä»»åŠ¡ï¼ˆå¯èƒ½æ˜¯æ”¶é›†ç»“æœç­‰ï¼‰ï¼Œç„¶ååˆ†å‘æ–°ä»»åŠ¡ã€‚åˆ†å‘æ–°ä»»åŠ¡çš„è¿‡ç¨‹åŒ…æ‹¬ï¼š
<ul>
<li><code>++node_n</code>ã€‚</li>
<li>ç­‰å¾…<code>src[0]</code>ï¼Œ <code>src[1]</code>å’Œ<code>src[2]</code>å®Œæˆã€‚</li>
<li>å¦‚æœæœ‰<code>INIT</code>å­ä»»åŠ¡ï¼Œæ‰§è¡Œ<code>INIT</code>å­ä»»åŠ¡ã€‚</li>
<li>å¦‚æœ<code>n_tasks == 1</code>ï¼Œè¯´æ˜åªæœ‰ä¸€ä»½ä»»åŠ¡ï¼Œç›´æ¥å½“åœºè‡ªå·±æ‰§è¡Œäº†ï¼Œä¸ç”¨äº¤ç»™å…¶ä»–çº¿ç¨‹ã€‚å¦‚æœæœ‰<code>FINALIZE</code>å­ä»»åŠ¡åˆ™æŠŠ<code>FINALIZE</code>ä¹Ÿæ‰§è¡Œäº†ã€‚ç„¶åå›åˆ°<code>++node_n</code>çš„æ­¥éª¤ã€‚</li>
<li><code>n_tasks</code>ä¸æ˜¯1ï¼Œé‚£ä¹ˆé‡ç½®å¥½<code>state-&gt;shared-&gt;n_active</code>ä¸º<code>n_threads</code>ï¼Œè®¾ç½®å¥½<code>state-&gt;shared-&gt;node_n</code>ä¸ºæ–°çš„<code>node_n</code>ã€‚</li>
</ul>
</li>
<li><code>atomic_fetch_sub(&amp;state-&gt;shared-&gt;n_active, 1) == 1</code>åˆ¤å®šå¤±è´¥çš„çº¿ç¨‹ä¼šå¯¹<code>state-&gt;shared-&gt;node_n</code>è¿›è¡Œç­‰å¾…ï¼Œç›´åˆ°å…¶å‡ºç°å˜åŒ–ï¼ˆå€¼ä¸ç­‰äºä¹‹å‰æ‹¿åˆ°çš„å±€éƒ¨å˜é‡<code>node_n</code>äº†ï¼‰ã€‚åˆ«å¿˜äº†åˆ†å‘ä»»åŠ¡çš„ç»“å°¾ä¼šè®¾ç½®æ–°çš„<code>state-&gt;shared-&gt;node_n</code>ï¼Œå…¶å½±å“çš„å°±æ˜¯è¿™é‡Œã€‚</li>
</ul>
</li>
<li>æ— è®ºæ˜¯å¦æ˜¯è´Ÿè´£åˆ†å‘ä»»åŠ¡çš„çº¿ç¨‹ï¼Œéƒ½ä¼šå°è¯•å¯¹å½“å‰ä»»åŠ¡è¿›è¡Œè®¡ç®—ã€‚
<ul>
<li><code>if (node_n &gt;= cgraph-&gt;n_nodes) break;</code>ï¼šæ‰€æœ‰èŠ‚ç‚¹æ‰§è¡Œå®Œæ¯•ï¼Œå¯ä»¥é€€å‡ºæœ€å¤–å±‚çš„<code>while(true)</code>å¾ªç¯ï¼Œä»»åŠ¡åœæ­¢ã€‚</li>
<li>è®¾ç½®<code>ggml_compute_params</code>ã€‚è¿™é‡Œç”±äºè´Ÿè´£æ‰§è¡ŒCPUè®¡ç®—çš„çº¿ç¨‹æ˜¯ä»<code>state-&gt;ith == 1</code>å¼€å§‹çš„ï¼Œå› æ­¤<code>ith</code>è¦å‡ä¸€ã€‚è‡³äº<code>nth</code>è¿™é‡Œä¸ºä»€ä¹ˆæ˜¯<code>n_tasks-1</code>ï¼Œæœ¬äººä¾ç„¶æœ‰ç–‘é—®ã€‚æœ‰çŸ¥é“çš„å¯è”ç³»ç¬”è€…æˆ–åœ¨è¯„è®ºåŒºè¯„è®ºã€‚</li>
<li><code>state-&gt;ith &lt; n_tasks</code>çš„æƒ…å†µä¸‹ï¼Œè°ƒç”¨<code>ggml_compute_forward</code>æ‰§è¡Œåˆ†é…ç»™è‡ªå·±çš„è®¡ç®—ä»»åŠ¡ã€‚</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre><code class="language-c++">static thread_ret_t ggml_graph_compute_thread_hybrid(void * data) {
    struct ggml_compute_state * state = (struct ggml_compute_state *) data;

    const struct ggml_cgraph * cgraph = state-&gt;shared-&gt;cgraph;
    const struct ggml_cplan  * cplan  = state-&gt;shared-&gt;cplan;

    const int   n_threads   = state-&gt;shared-&gt;n_threads;

    set_numa_thread_affinity(state-&gt;ith, n_threads);

    // cpu_set_t mask;
    // CPU_ZERO(&amp;mask);
    // CPU_SET(state-&gt;ith * 2, &amp;mask);
    // if (sched_setaffinity(0, sizeof(mask), &amp;mask) == -1) {
    //     perror(&quot;sched_setaffinity&quot;);
    // }

    int node_n = -1;

    while (true) {
        if (cplan-&gt;abort_callback &amp;&amp; cplan-&gt;abort_callback(cplan-&gt;abort_callback_data)) {
            state-&gt;shared-&gt;node_n += 1;
            return (thread_ret_t) GGML_EXIT_ABORTED;
        }
        if (state-&gt;ith == 0)
        {
            // atomic_fetch_sub(&amp;state-&gt;shared-&gt;n_active, 1);
            node_n = -1;
            // return 0;

            while (1)
            {
                state-&gt;shared-&gt;perf_node_start_cycles  = ggml_perf_cycles();
                state-&gt;shared-&gt;perf_node_start_time_us = ggml_perf_time_us();
                node_n = node_n + 1;
                if (node_n &gt;= cgraph-&gt;n_nodes)
                    return 0;
                struct ggml_tensor *node = cgraph-&gt;nodes[node_n];
                if (node-&gt;backend == GGML_BACKEND_CPU)
                    continue;
                // uint64_t dbug = 0;
                while (1)
                {
                    // dbug++;
                    int status0 = atomic_load(&amp;node-&gt;src[0]-&gt;is_finish);
                    int status1 = 1;
                    int status2 = 1;
                    if (node-&gt;src[1] != NULL)
                        status1 = atomic_load(&amp;node-&gt;src[1]-&gt;is_finish);
                    if (node-&gt;src[2] != NULL)
                        status2 = atomic_load(&amp;node-&gt;src[2]-&gt;is_finish);
                    // if (dbug &gt; 10000000) {
                    //     printf(&quot;stuck %s thread %d\n&quot;, ggml_get_name(node), n_threads);
                    //     int k;
                    //     scanf(&quot;%d&quot;, &amp;k);
                    // }
                    if (status0 == 1 &amp;&amp; status1 == 1 &amp;&amp; status2 == 1)
                    {
                        break;
                    }
                    // else
                    //     busy_wait_cycles(10);
                }
                struct ggml_compute_params params = {
                    /*.type  =*/GGML_TASK_COMPUTE,
                    /*.ith   =*/0,
                    /*.nth   =*/1,
                    /*.wsize =*/0,
                    /*.wdata =*/0,
                    /*.aic   =*/0,
                };


                // printf(&quot;GPU %s\n&quot;, ggml_get_name(node));
                // cudaDeviceSynchronize();
                ggml_compute_forward(&amp;params, node);
                // cudaDeviceSynchronize();
                // ggml_graph_compute_perf_stats_node_gpu(node, state-&gt;shared);
                ggml_graph_compute_perf_stats_node_gpu(node, state-&gt;shared);
                // if (strcmp(ggml_get_name(node), &quot;before&quot;) == 0)
                //     printf(&quot;%ld\n&quot;, ggml_time_us());
                atomic_store(&amp;node-&gt;is_finish, 1);
            }
        }
        if (atomic_fetch_sub(&amp;state-&gt;shared-&gt;n_active, 1) == 1) {
            // all other threads are finished and spinning
            // do finalize and init here so we don't have synchronize again
            struct ggml_compute_params params = {
                /*.type  =*/ GGML_TASK_FINALIZE,
                /*.ith   =*/ 0,
                /*.nth   =*/ 0,
                /*.wsize =*/ cplan-&gt;work_size,
                /*.wdata =*/ cplan-&gt;work_data,
                /*.aic   =*/ &amp;state-&gt;shared-&gt;aic,
            };

            if (node_n != -1) {
                /* FINALIZE */
                struct ggml_tensor * node = cgraph-&gt;nodes[node_n];
                if (GGML_OP_HAS_FINALIZE[node-&gt;op]) {
                    params.nth = ggml_get_n_tasks(node, n_threads);
                    ggml_compute_forward(&amp;params, node);
                }
                ggml_graph_compute_perf_stats_node(node, state-&gt;shared);
                atomic_store(&amp;node-&gt;is_finish, 1);
            }

            // distribute new work or execute it direct if 1T
            while (++node_n &lt; cgraph-&gt;n_nodes) {
                GGML_PRINT_DEBUG_5(&quot;%s: %d/%d\n&quot;, __func__, node_n, cgraph-&gt;n_nodes);

                struct ggml_tensor * node = cgraph-&gt;nodes[node_n];
                const int n_tasks = ggml_get_n_tasks(node, n_threads);

                state-&gt;shared-&gt;perf_node_start_cycles  = ggml_perf_cycles();
                state-&gt;shared-&gt;perf_node_start_time_us = ggml_perf_time_us();

                params.nth = n_tasks;
                if (node-&gt;backend == GGML_BACKEND_GPU)
                    continue;
                while(1)
                {
                    int status0 = atomic_load(&amp;node-&gt;src[0]-&gt;is_finish);
                    int status1 = 1;
                    int status2 = 1;
                    if(node-&gt;src[1] != NULL)
                        status1 = atomic_load(&amp;node-&gt;src[1]-&gt;is_finish);
                    if(node-&gt;src[2] != NULL)
                        status2 = atomic_load(&amp;node-&gt;src[2]-&gt;is_finish);
                    if(status0 == 1 &amp;&amp; status1 == 1 &amp;&amp; status2 == 1)
                        break;
                    // else busy_wait_cycles(10);
                }

                /* INIT */
                if (GGML_OP_HAS_INIT[node-&gt;op]) {
                    params.type = GGML_TASK_INIT;
                    ggml_compute_forward(&amp;params, node);
                }

                if (n_tasks == 1) {
                    // TODO: maybe push node_n to the atomic but if other threads see n_tasks is 1,
                    // they do something more efficient than spinning (?)
                    params.type = GGML_TASK_COMPUTE;
                    ggml_compute_forward(&amp;params, node);
                    atomic_store(&amp;node-&gt;is_finish, 1);

                    if (GGML_OP_HAS_FINALIZE[node-&gt;op]) {
                        params.type = GGML_TASK_FINALIZE;
                        ggml_compute_forward(&amp;params, node);
                    }

                    ggml_graph_compute_perf_stats_node(node, state-&gt;shared);
                } else {
                    break;
                }

                if (cplan-&gt;abort_callback &amp;&amp; cplan-&gt;abort_callback(cplan-&gt;abort_callback_data)) {
                    break;
                }
            }

            atomic_store(&amp;state-&gt;shared-&gt;n_active, n_threads);
            atomic_store(&amp;state-&gt;shared-&gt;node_n,   node_n);
        } else {
            // wait for other threads to finish
            const int last = node_n;
            while (true) {
                // TODO: this sched_yield can have significant impact on the performance - either positive or negative
                //       depending on the workload and the operating system.
                //       since it is not clear what is the best approach, it should potentially become user-configurable
                //       ref: https://github.com/ggerganov/ggml/issues/291
#if defined(GGML_USE_ACCELERATE) || defined(GGML_USE_OPENBLAS)
                sched_yield();
#endif

                node_n = atomic_load(&amp;state-&gt;shared-&gt;node_n);
                if (node_n != last) break;
            };
        }

        // check if we should stop
        if (node_n &gt;= cgraph-&gt;n_nodes) break;

        /* COMPUTE */
        struct ggml_tensor * node = cgraph-&gt;nodes[node_n];
        const int n_tasks = ggml_get_n_tasks(node, n_threads);

        struct ggml_compute_params params = {
            /*.type  =*/ GGML_TASK_COMPUTE,
            /*.ith   =*/ state-&gt;ith-1,
            /*.nth   =*/ n_tasks-1,
            /*.wsize =*/ cplan-&gt;work_size,
            /*.wdata =*/ cplan-&gt;work_data,
            /*.aic   =*/ &amp;state-&gt;shared-&gt;aic,
        };

        if (state-&gt;ith &lt; n_tasks) {
            ggml_compute_forward(&amp;params, node);
        }
    }

    return GGML_EXIT_SUCCESS;
}
</code></pre>
<p><code>ggml_compute_forward</code>ä¼šè´Ÿè´£æ ¹æ®<code>tensor-&gt;op</code>å»è°ƒç”¨å…·ä½“çš„ç®—å­ã€‚</p>
<pre><code class="language-c++">static void ggml_compute_forward(struct ggml_compute_params * params, struct ggml_tensor * tensor) {
    GGML_ASSERT(params);

    if (tensor-&gt;op == GGML_OP_NONE) {
        return;
    }

#ifdef GGML_USE_CUBLAS
    bool skip_cpu = ggml_cuda_compute_forward(params, tensor);
    if (skip_cpu) {
        return;
    }
    // Make sure src[0] (weight for binary ops) is on CPU to avoid any weight transfer
    GGML_ASSERT((tensor-&gt;src[0] == NULL || tensor-&gt;src[0]-&gt;backend == GGML_BACKEND_CPU) &amp;&amp; &quot;weight should be on the CPU to compute on the CPU&quot;);
#endif // GGML_USE_CUBLAS

    switch (tensor-&gt;op) {
        case GGML_OP_DUP:
            {
                ggml_compute_forward_dup(params, tensor-&gt;src[0], tensor);
            } break;
        case GGML_OP_ADD:
            {
                ggml_compute_forward_add(params, tensor-&gt;src[0], tensor-&gt;src[1], tensor);
            } break;
        case GGML_OP_ADD1:
            {
                ggml_compute_forward_add1(params, tensor-&gt;src[0], tensor-&gt;src[1], tensor);
            } break;
        case GGML_OP_ACC:
            {
                ggml_compute_forward_acc(params, tensor-&gt;src[0], tensor-&gt;src[1], tensor);
            } break;
        case GGML_OP_SUB:
            {
                ggml_compute_forward_sub(params, tensor-&gt;src[0], tensor-&gt;src[1], tensor);
            } break;
        case GGML_OP_MUL:
            {
                ggml_compute_forward_mul(params, tensor-&gt;src[0], tensor-&gt;src[1], tensor);
            } break;
        case GGML_OP_DIV:
            {
                ggml_compute_forward_div(params, tensor-&gt;src[0], tensor-&gt;src[1], tensor);
            } break;
            // Other cases...
            // -- snip --
    }
}
</code></pre>
<p>é¢å¤–åœ°ï¼Œæˆ‘ä»¬æ³¨æ„<code>ggml_compute_foward</code>çš„å¼€å¤´ï¼šå¦‚æœä½¿ç”¨äº†CUBLASï¼Œé‚£ä¹ˆå°±è½¬è€Œè°ƒç”¨CUDAå®ç°çš„ç®—å­ï¼ˆ<code>ggml_cuda_compute_forward</code>ï¼‰ã€‚</p>
<pre><code class="language-c++">static void ggml_compute_forward(struct ggml_compute_params * params, struct ggml_tensor * tensor) {
    GGML_ASSERT(params);

    if (tensor-&gt;op == GGML_OP_NONE) {
        return;
    }

#ifdef GGML_USE_CUBLAS
    bool skip_cpu = ggml_cuda_compute_forward(params, tensor);
    if (skip_cpu) {
        return;
    }
    // Make sure src[0] (weight for binary ops) is on CPU to avoid any weight transfer
    GGML_ASSERT((tensor-&gt;src[0] == NULL || tensor-&gt;src[0]-&gt;backend == GGML_BACKEND_CPU) &amp;&amp; &quot;weight should be on the CPU to compute on the CPU&quot;);
#endif // GGML_USE_CUBLAS

    switch (tensor-&gt;op) {
        case GGML_OP_DUP:
            {
                ggml_compute_forward_dup(params, tensor-&gt;src[0], tensor);
            } break;
            // Other cases...
            // -- snip --
    }
}
</code></pre>
<p>åœ¨<code>ggml_cuda_compute_forward</code>ä¸­ï¼Œä¼šæ ¹æ®<code>tensor-&gt;op</code>å»è°ƒç”¨å…·ä½“çš„CUDAç®—å­ã€‚</p>
<pre><code class="language-c++">bool ggml_cuda_compute_forward(struct ggml_compute_params * params, struct ggml_tensor * tensor) {
    if (!g_cublas_loaded) return false;

    ggml_cuda_func_t func;
    const bool src0_on_device = tensor-&gt;src[0] != nullptr &amp;&amp; (tensor-&gt;src[0]-&gt;backend != GGML_BACKEND_CPU);
    const bool any_on_device = tensor-&gt;backend == GGML_BACKEND_GPU || src0_on_device
        || (tensor-&gt;src[1] != nullptr &amp;&amp; tensor-&gt;src[1]-&gt;backend == GGML_BACKEND_GPU);

    // when src0 (weights) is not on device, we compute on CPU with sparsity
    if (!src0_on_device &amp;&amp; (tensor-&gt;op == GGML_OP_MUL_MAT_SPARSE || tensor-&gt;op == GGML_OP_AXPY)
        || !any_on_device &amp;&amp; tensor-&gt;op != GGML_OP_MUL_MAT) {
        return false;
    }

    if (tensor-&gt;op == GGML_OP_MUL_MAT) {
        if (tensor-&gt;src[0]-&gt;ne[3] != tensor-&gt;src[1]-&gt;ne[3]) {
#ifndef NDEBUG
            fprintf(stderr, &quot;%s: cannot compute %s: src0-&gt;ne[3] = %d, src1-&gt;ne[3] = %d - fallback to CPU\n&quot;, __func__, tensor-&gt;name, tensor-&gt;src[0]-&gt;ne[3], tensor-&gt;src[1]-&gt;ne[3]);
#endif
            return false;
        }
    }

    switch (tensor-&gt;op) {
        case GGML_OP_REPEAT:
            func = ggml_cuda_repeat;
            break;
        case GGML_OP_GET_ROWS:
            func = ggml_cuda_get_rows;
            break;
        case GGML_OP_DUP:
            func = ggml_cuda_dup;
            break;
        case GGML_OP_ADD:
            func = ggml_cuda_add;
            break;
        case GGML_OP_MUL:
            func = ggml_cuda_mul;
            break;
        case GGML_OP_UNARY:
            switch (ggml_get_unary_op(tensor)) {
                case GGML_UNARY_OP_GELU:
                    func = ggml_cuda_gelu;
                    break;
                case GGML_UNARY_OP_SILU:
                    func = ggml_cuda_silu;
                    break;
                case GGML_UNARY_OP_RELU:
                    func = ggml_cuda_relu;
                    break;
                default:
                    return false;
            } break;
        case GGML_OP_NORM:
            func = ggml_cuda_norm;
            break;
        case GGML_OP_RMS_NORM:
            func = ggml_cuda_rms_norm;
            break;
        case GGML_OP_MUL_MAT:
            if (!any_on_device &amp;&amp; !ggml_cuda_can_mul_mat(tensor-&gt;src[0], tensor-&gt;src[1], tensor)) {
                return false;
            }
            func = ggml_cuda_mul_mat;
            break;
        case GGML_OP_MUL_MAT_SPARSE:
            if (!src0_on_device &amp;&amp; !ggml_cuda_can_mul_mat(tensor-&gt;src[0], tensor-&gt;src[1], tensor)) {
                return false;
            }
            func = ggml_cuda_mul_mat_sparse;
            break;
        case GGML_OP_AXPY:
            func = ggml_cuda_axpy;
            break;
        // Other cases...
        // -- snip --
        default:
            return false;
    }

    if (params-&gt;ith != 0) {
        return true;
    }
    if (params-&gt;type == GGML_TASK_INIT || params-&gt;type == GGML_TASK_FINALIZE) {
        return true;
    }
    func(tensor-&gt;src[0], tensor-&gt;src[1], tensor);

    // CUDA_CHECK(cudaDeviceSynchronize());

    return true;
}
</code></pre>
<p>ä»¥<code>ggml_cuda_mul_mat_sparse</code>ä¸ºä¾‹ï¼Œå…¶ä¼šæ ¹æ®ç±»å‹ç»§ç»­ä¸‹è°ƒã€‚</p>
<pre><code class="language-c++">static void ggml_cuda_mul_mat_sparse(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {
    GGML_ASSERT(dst-&gt;src[2] != NULL &amp;&amp; &quot;dst-&gt;src[2] must be present for sparse matrix multiplication&quot;);
    if (src1-&gt;ne[1] == 1 &amp;&amp; src0-&gt;ne[0] % GGML_CUDA_DMMV_X == 0) {
        switch(src0-&gt;type) {
            case GGML_TYPE_F16:
                ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_mul_mat_vec_sparse_dequantized, false);
                break;
            case GGML_TYPE_Q4_0:
                ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_mul_mat_vec_sparse_q, true);
                break;
            default:
                GGML_ASSERT(false &amp;&amp; &quot;unsupported type for sparse matrix multiplication&quot;);
        }
    } else {
        ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_mul_mat_batch_sparse, false);
    }
}
</code></pre>
<p>è‡³æ­¤ï¼Œåç»­ä¾¿èƒ½è°ƒç”¨åˆ°å…·ä½“è´Ÿè´£è®¡ç®—çš„ç®—å­ã€‚æ¨¡å‹åŠ è½½ä»¥åï¼Œè®¡ç®—å›¾çš„æ„å»ºä¸€ç›´åˆ°ç®—å­çš„è°ƒç”¨çš„å¤§è‡´æµç¨‹ä¹Ÿç»“æŸäº†ã€‚</p>
<h2 id="æ”¯æŒ-ï¸">æ”¯æŒ â˜•ï¸</h2>
<p>å¦‚æœå‘ç°å†…å®¹æœ‰çº°æ¼æˆ–é”™è¯¯ï¼Œå¯ä»¥é€šè¿‡é‚®ç®±hangyu.yuan@qq.comè”ç³»æˆ‘æˆ–ç›´æ¥åœ¨ä¸‹æ–¹è¯„è®ºå‘Šè¯‰æˆ‘ï¼Œè°¢è°¢ã€‚<br>
æˆ‘çš„<a href="https://github.com/Yuan-Allen">GitHubä¸»é¡µ</a>ã€‚</p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#%E7%9B%B8%E5%85%B3">ç›¸å…³</a></li>
<li><a href="#%E8%AE%A1%E7%AE%97%E5%9B%BE%E6%9E%84%E5%BB%BA">è®¡ç®—å›¾æ„å»º</a></li>
<li><a href="#%E7%AE%97%E5%AD%90%E8%B0%83%E7%94%A8">ç®—å­è°ƒç”¨</a></li>
<li><a href="#%E6%94%AF%E6%8C%81-%EF%B8%8F">æ”¯æŒ â˜•ï¸</a></li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">ä¸‹ä¸€ç¯‡</div>
            <a href="https://alleny.xyz/post/leetcode-binary-tree/">
              <h3 class="post-title">
                LeetCodeåˆ·é¢˜è®°å½•ï¼ˆäºŒå‰æ ‘ç¯‡ï¼‰
              </h3>
            </a>
          </div>
        

        
          

          
            <link rel="stylesheet" href="https://unpkg.com/disqusjs@1.1/dist/disqusjs.css">
<script src="https://unpkg.com/disqusjs@1.1/dist/disqus.js"></script>

<div id="disqus_thread"></div>

<script>

var options = {
  shortname: 'alleny-blog',
  apikey: '2kPJh2tQ0rWB7n0QOhvb9TLbm946tHMWCixW2qGF9j8tfMBgZoNPfvLpDoxZTZpD',
}
if ('https://disqus.hangyu-yuan.workers.dev/api/') {
  options.api = 'https://disqus.hangyu-yuan.workers.dev/api/'
}
var dsqjs = new DisqusJS(options)

</script>

          
        

        <div class="site-footer">
  Copyright Â© 2023-2025 Allen Yuan. All rights reserved.
  <a class="rss" href="https://alleny.xyz/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
