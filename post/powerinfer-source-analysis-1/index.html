<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>PowerInferæºç è§£æï¼ˆä¸€ï¼‰ï¼šæ¨¡å‹åŠ è½½ | AllenY&#39;s blog</title>
<link rel="shortcut icon" href="https://alleny.xyz/favicon.ico?v=1747120786321">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://alleny.xyz/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="PowerInferæºç è§£æï¼ˆä¸€ï¼‰ï¼šæ¨¡å‹åŠ è½½ | AllenY&#39;s blog - Atom Feed" href="https://alleny.xyz/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">


<script async src="https://www.googletagmanager.com/gtag/js?id=G-HPLP8D1S43"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-HPLP8D1S43');
</script>


    <meta name="description" content="æœ€è¿‘å¼€å§‹ç€æ‰‹ç ”ç©¶PowerInferï¼Œä½†çœ‹äº†ä¸‹ç½‘ä¸Šä¼¼ä¹è¿˜æ²¡æœ‰é’ˆå¯¹PowerInferçš„ä»£ç è§£æï¼Œå› æ­¤æœ¬äººæ‰“ç®—è‡ªå·±åŠ¨æ‰‹ã€‚

ç›¸å…³

PowerInferæºç è§£æï¼ˆäºŒï¼‰ï¼šè®¡ç®—å›¾æ„å»ºä¸ç®—å­è°ƒç”¨ ã€‚
PowerInferæºç è§£æï¼ˆä¸‰ï¼‰ï¼šç®—å­å®ç°ã€‚
..." />
    <meta name="keywords" content="System for AI,Technical Sharing" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.5.1/build/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://alleny.xyz">
  <img class="avatar" src="https://alleny.xyz/images/avatar.png?v=1747120786321" alt="">
  </a>
  <h1 class="site-title">
    AllenY&#39;s blog
  </h1>
  <p class="site-description">
    ğŸ§‘ğŸ»â€ğŸ’»ğŸ®ğŸ¿ğŸ¹ğŸ—»
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          Home
        </a>
      
    
      
        <a href="/archives" class="menu">
          Archive
        </a>
      
    
      
        <a href="/tags" class="menu">
          Tags
        </a>
      
    
      
        <a href="/post/about" class="menu">
          About
        </a>
      
    
  </div>
  <div class="social-container">
    
      
        <a href="https://github.com/Yuan-Allen" target="_blank">
          <i class="ri-github-line"></i>
        </a>
      
    
      
    
      
        <a href="https://weibo.com/u/6478080851" target="_blank">
          <i class="ri-weibo-line"></i>
        </a>
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              PowerInferæºç è§£æï¼ˆä¸€ï¼‰ï¼šæ¨¡å‹åŠ è½½
            </h2>
            <div class="post-info">
              <span>
                2024-09-24
              </span>
              <span>
                20 min read
              </span>
              
                <a href="https://alleny.xyz/tag/system-for-ai/" class="post-tag">
                  # System for AI
                </a>
              
                <a href="https://alleny.xyz/tag/WT-UATSMl/" class="post-tag">
                  # Technical Sharing
                </a>
              
            </div>
            
              <img class="post-feature-image" src="https://alleny.xyz/post-images/powerinfer-source-analysis-1.png" alt="">
            
            <div class="post-content-wrapper">
              <div class="post-content" v-pre>
                <p>æœ€è¿‘å¼€å§‹ç€æ‰‹ç ”ç©¶PowerInferï¼Œä½†çœ‹äº†ä¸‹ç½‘ä¸Šä¼¼ä¹è¿˜æ²¡æœ‰é’ˆå¯¹PowerInferçš„ä»£ç è§£æï¼Œå› æ­¤æœ¬äººæ‰“ç®—è‡ªå·±åŠ¨æ‰‹ã€‚</p>
<!-- more -->
<h2 id="ç›¸å…³">ç›¸å…³</h2>
<ul>
<li><a href="https://alleny.xyz/post/powerinfer-source-analysis-2/">PowerInferæºç è§£æï¼ˆäºŒï¼‰ï¼šè®¡ç®—å›¾æ„å»ºä¸ç®—å­è°ƒç”¨ </a>ã€‚</li>
<li><a href="https://alleny.xyz/post/powerinfer-source-analysis-3/">PowerInferæºç è§£æï¼ˆä¸‰ï¼‰ï¼šç®—å­å®ç°</a>ã€‚</li>
</ul>
<h2 id="èƒŒæ™¯">èƒŒæ™¯</h2>
<p>PowerInferæ˜¯ç”±è‘—åçš„åŒæ ¡åŒé™¢ç³»å®éªŒå®¤ï¼ˆ<a href="https://ipads.se.sjtu.edu.cn/zh/index.html">IPADS</a>ï¼‰æ¨å‡ºçš„åœ¨é…å¤‡å•ä¸ªæ¶ˆè´¹çº§GPUçš„ä¸ªäººç”µè„‘ï¼ˆPCï¼‰ä¸Šè¿›è¡Œé«˜é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†çš„å¼•æ“ã€‚PowerInferè®¾è®¡çš„å…³é”®åœ¨äºåˆ©ç”¨LLMæ¨ç†ä¸­å›ºæœ‰çš„é«˜å±€éƒ¨æ€§ç‰¹å¾ï¼Œè¯¥ç‰¹å¾è¡¨ç°ä¸ºç¥ç»å…ƒæ¿€æ´»çš„å¹‚å¾‹åˆ†å¸ƒï¼ˆa power-law distribution in neuron activationï¼‰ã€‚è¯¥åˆ†å¸ƒè¡¨æ˜ï¼Œä¸€å°éƒ¨åˆ†ç¥ç»å…ƒï¼Œè¢«ç§°ä¸ºçƒ­ç¥ç»å…ƒï¼ˆhot neuronsï¼‰ï¼Œåœ¨ä¸åŒçš„è¾“å…¥ä¸­å§‹ç»ˆä¿æŒæ¿€æ´»çŠ¶æ€ï¼Œè€Œå¤§å¤šæ•°ç¥ç»å…ƒï¼ˆå†·ç¥ç»å…ƒï¼Œcold neuronsï¼‰åˆ™æ ¹æ®ç‰¹å®šè¾“å…¥å‘ç”Ÿå˜åŒ–ã€‚PowerInferåˆ©ç”¨è¿™ä¸€è§è§£è®¾è®¡äº†ä¸€ä¸ªGPU-CPUæ··åˆæ¨ç†å¼•æ“ï¼šçƒ­ç¥ç»å…ƒé¢„å…ˆåŠ è½½åˆ°GPUä¸­ä»¥ä¾¿å¿«é€Ÿè®¿é—®ï¼Œè€Œå†·ç¥ç»å…ƒåˆ™åœ¨CPUä¸Šè¿›è¡Œè®¡ç®—ï¼Œä»è€Œæ˜¾è‘—å‡å°‘äº†GPUå†…å­˜éœ€æ±‚å’ŒCPU-GPUä¹‹é—´çš„æ•°æ®ä¼ è¾“ã€‚PowerInferè¿˜é›†æˆäº†è‡ªé€‚åº”é¢„æµ‹å™¨ï¼ˆadaptive predictorsï¼‰å’Œç¥ç»å…ƒæ„ŸçŸ¥ç¨€ç–ç®—å­ï¼ˆneuron-aware sparse operatorsï¼‰ï¼Œä¼˜åŒ–äº†ç¥ç»å…ƒæ¿€æ´»å’Œè®¡ç®—ç¨€ç–æ€§çš„æ•ˆç‡ã€‚è¯„ä¼°ç»“æœæ˜¾ç¤ºï¼ŒPowerInferåœ¨å•ä¸ªNVIDIA RTX 4090 GPUä¸Šå¯¹å„ç§LLMï¼ˆåŒ…æ‹¬OPT-175Bï¼‰å®ç°äº†å¹³å‡æ¯ç§’ç”Ÿæˆ13.20ä¸ªtokenï¼Œå³°å€¼è¾¾åˆ°29.08ä¸ªtoken/sï¼Œä»…æ¯”é¡¶çº§æœåŠ¡å™¨çº§A100 GPUä½18%ã€‚åŒæ—¶ï¼ŒPowerInferçš„æ€§èƒ½è¿œè¶…llama.cppï¼Œæœ€é«˜å¯è¾¾11.69å€ï¼Œå¹¶ä¿æŒæ¨¡å‹ç²¾åº¦ã€‚</p>
<h2 id="è¯´æ˜">è¯´æ˜</h2>
<ul>
<li><a href="https://github.com/SJTU-IPADS/PowerInfer">PowerInferçš„ä»“åº“åœ°å€</a>ã€‚</li>
<li>PowerInferåŸºäº<a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>çš„Commit <code>6bb4908a17150b49373b5f977685b2e180a04f6f</code> è¿›è¡Œä¿®æ”¹ã€‚llama.cppçš„ä»£ç é‡ä¸å°‘ï¼Œç»´æŠ¤è¾ƒå‹¤ï¼Œç½‘ä¸Šä¹Ÿæœ‰å¾ˆå¤šè§£æï¼Œå› æ­¤ä¸æ˜¯æœ¬æ–‡è¦ä»‹ç»çš„é‡ç‚¹ã€‚<strong>æœ¬æ–‡ä¸»è¦ä»‹ç»çš„æ˜¯PowerInferåŸºäºllama.cppçš„ä¿®æ”¹éƒ¨åˆ†,å¯¹llama.cppçš„åŸæœ‰éƒ¨åˆ†ä¸ä¼šæœ‰è¿‡å¤šè§£é‡Šã€‚</strong></li>
<li>æœ¬æ–‡æ‰€ä½¿ç”¨æ¥å¯¹æ¯”çš„PowerInfer Commitï¼š <code>6ae7e06dbe1032ec103e1a08ce126b3d1ed2d6e7</code> &lt;--&gt; <code>6bb4908a17150b49373b5f977685b2e180a04f6f</code></li>
</ul>
<h2 id="ggml">GGML</h2>
<blockquote>
<p>æ•´ä¸ªllama.cppé¡¹ç›®å¯ä»¥åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šåº•å±‚çš„å¼ é‡åº“GGML(Cè¯­è¨€)ï¼Œå’Œåº”ç”¨å±‚çš„æ¨¡å‹æ¨ç†ä»£ç (C++è¯­è¨€)ã€‚ä¸¥æ ¼æ¥è¯´ï¼ŒGGMLæ˜¯ä¸€ä¸ªç‹¬ç«‹çš„é¡¹ç›®ï¼Œä½†åœ¨å®é™…å¼€å‘ä¸­ï¼ŒGGMLè¢«å®Œæ•´åŒ…å«åœ¨llama.cppé¡¹ç›®ä¸­(å·¥ç¨‹ç›®å½•ä¸‹çš„ggml*æ–‡ä»¶)ä¸€èµ·å¼€å‘ï¼Œå¹¶åé¦ˆåˆå¹¶ç»™ä¸Šæ¸¸çš„åŸä»“åº“ã€‚</p>
</blockquote>
<p>ggml æ˜¯ä¸€ä¸ªç”¨ C å’Œ C++ ç¼–å†™ã€ä¸“æ³¨äº Transformer æ¶æ„æ¨¡å‹æ¨ç†çš„æœºå™¨å­¦ä¹ åº“ã€‚è¯¥é¡¹ç›®å®Œå…¨å¼€æºï¼Œå¤„äºæ´»è·ƒçš„å¼€å‘é˜¶æ®µï¼Œå¼€å‘ç¤¾åŒºä¹Ÿåœ¨ä¸æ–­å£®å¤§ã€‚ggml å’Œ PyTorchã€TensorFlow ç­‰æœºå™¨å­¦ä¹ åº“æ¯”è¾ƒç›¸ä¼¼ï¼Œä½†ç”±äºç›®å‰å¤„äºå¼€å‘çš„æ—©æœŸé˜¶æ®µï¼Œä¸€äº›åº•å±‚è®¾è®¡ä»åœ¨ä¸æ–­æ”¹è¿›ä¸­ã€‚</p>
<p>å…³äºggmlçš„ä½¿ç”¨ï¼Œå¯å‚è€ƒ<a href="https://huggingface.co/blog/zh/introduction-to-ggml">è¿™ç¯‡blog</a>ä»¥æœ‰ä¸€ä¸ªåŸºæœ¬çš„äº†è§£ã€‚</p>
<h2 id="æ­£æ–‡">æ­£æ–‡</h2>
<p>ç›®å…‰è½¬å‘ç¨‹åºçš„å…¥å£ï¼Œ<code>example/main/main.cpp:main</code>ã€‚ç»è¿‡ä¸€å †ä¹±ä¸ƒå…«ç³Ÿçš„å‚æ•°è§£æï¼Œæˆ‘ä»¬æŠŠç›®å…‰é”å®šåœ¨è¿™é‡Œï¼š</p>
<pre><code class="language-c++">    // load the model and apply lora adapter, if any
    LOG(&quot;%s: load the model and apply lora adapter, if any\n&quot;, __func__);
    std::tie(model, ctx) = llama_init_from_gpt_params(params);
    if (sparams.cfg_scale &gt; 1.f) {
        struct llama_context_params lparams = llama_context_params_from_gpt_params(params);
        ctx_guidance = llama_new_context_with_model(model, lparams);
    }
</code></pre>
<p>ç‚¹è¿›è¿™é‡Œè°ƒç”¨çš„<code>llama_init_from_gpt_params</code>ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°æ›´æ”¹çš„éƒ¨åˆ†ã€‚åŸå…ˆï¼Œ<code>cparams</code>æ˜¯åœ¨<code>llama_load_model_from_file</code>ä¹‹åæ‰ç”Ÿæˆçš„ï¼Œè€Œè¿™é‡Œè¢«æåˆ°äº†å‰é¢ï¼Œå¹¶ä½œä¸ºå‚æ•°ä¼ ç»™äº†<code>llama_load_model_from_file_with_context</code>ï¼Œè¯´æ˜PowerInferçš„åˆå§‹åŒ–è¿‡ç¨‹å¯¹<code>cparams</code>æœ‰é¢å¤–çš„éœ€æ±‚ã€‚</p>
<pre><code class="language-c++">std::tuple&lt;struct llama_model *, struct llama_context *&gt; llama_init_from_gpt_params(gpt_params &amp; params) {
    auto mparams = llama_model_params_from_gpt_params(params);
    auto cparams = llama_context_params_from_gpt_params(params);

    llama_model * model  = llama_load_model_from_file_with_context(params.model.c_str(), mparams, &amp;cparams);
    if (model == NULL) {
        fprintf(stderr, &quot;%s: error: failed to load model '%s'\n&quot;, __func__, params.model.c_str());
        return std::make_tuple(nullptr, nullptr);
    }
</code></pre>
<p>æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒåŸå…ˆçš„<code>llama_load_model_from_file</code>æ¥å£å¾—åˆ°ä¿ç•™ï¼Œå…¶ä¹Ÿæ˜¯è°ƒåˆ°äº†PowerInferæ–°åŠ çš„<code>llama_load_model_from_file_with_context</code>æ¥å£ï¼Œåªè¦æŠŠé¢å¤–çš„<code>cparams</code>å‚æ•°è®¾ç½®ä¸º<code>nullptr</code>å³å¯ã€‚</p>
<pre><code class="language-c++">struct llama_model * llama_load_model_from_file(
                             const char * path_model,
              struct llama_model_params   params) {
    return llama_load_model_from_file_with_context(path_model, params, nullptr);
}
</code></pre>
<p>è€Œ<code>llama_load_model_from_file_with_context</code>åˆ™æ˜¯æ›¿ä»£äº†åŸå…ˆæ¥å£çš„åŠŸèƒ½ã€‚é¢å¤–åœ°ï¼Œå…¶æŠŠ<code>cparams</code>å‚æ•°åˆä¼ ç»™äº†<code>llama_model_load</code>ã€‚</p>
<pre><code class="language-c++">struct llama_model * llama_load_model_from_file_with_context(
    const char * path_model,
    struct llama_model_params   params,
    struct llama_context_params * cparams
) {
    ggml_time_init();

    llama_model * model = new llama_model;

    unsigned cur_percentage = 0;
    if (params.progress_callback == NULL) {
        params.progress_callback_user_data = &amp;cur_percentage;
        params.progress_callback = [](float progress, void * ctx) {
            unsigned * cur_percentage_p = (unsigned *) ctx;
            unsigned percentage = (unsigned) (100 * progress);
            while (percentage &gt; *cur_percentage_p) {
                *cur_percentage_p = percentage;
                LLAMA_LOG_INFO(&quot;.&quot;);
                if (percentage &gt;= 100) {
                    LLAMA_LOG_INFO(&quot;\n&quot;);
                }
            }
        };
    }

    if (!llama_model_load(path_model, *model, params, cparams)) {
        LLAMA_LOG_ERROR(&quot;%s: failed to load model\n&quot;, __func__);
        delete model;
        return nullptr;
    }

    return model;
}
</code></pre>
<p>è¿›å…¥<code>llama_model_load</code>é‡Œé¢ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°PowerInferå…ˆä½¿ç”¨<code>params</code>åˆå§‹åŒ–äº†<code>llama_model_loader</code>ï¼Œè¯¥ç±»å‹å¯¹è±¡åœ¨æ„é€ æ—¶ä¼šè¯»å–<code>fname</code>æŒ‡å®šçš„<code>gguf</code>æ–‡ä»¶ï¼Œè§£ææ–‡ä»¶çš„å†…å®¹ï¼Œå¹¶æ ¹æ®æ–‡ä»¶å†…å®¹è®¾ç½®ä¸€äº›ä¿¡æ¯ã€‚</p>
<pre><code class="language-c++">static bool llama_model_load(const std::string &amp; fname, llama_model &amp; model, const llama_model_params &amp; params, const llama_context_params * cparams) {
    try {
        llama_model_loader ml(fname, params.use_mmap);

        if (ml.sparse_deriv == GGML_SPARSE_INFERENCE) {
            LLAMA_LOG_INFO(&quot;%s: PowerInfer model loaded. Sparse inference will be used.\n&quot;, __func__);
        }

        model.hparams.vocab_only = params.vocab_only;
        model.sparse_deriv = ml.sparse_deriv;

        llm_load_arch   (ml, model);
        llm_load_hparams(ml, model);
        llm_load_vocab  (ml, model);

        llm_load_print_meta(ml, model);

        if (model.hparams.n_vocab != model.vocab.id_to_token.size()) {
            throw std::runtime_error(&quot;vocab size mismatch&quot;);
        }

        if (params.vocab_only) {
            LLAMA_LOG_INFO(&quot;%s: vocab only - skipping tensors\n&quot;, __func__);
            return true;
        }

        if (llama_use_sparse_inference(&amp;model)) {
            if (params.n_gpu_layers &gt; 0) {
                LLAMA_LOG_WARN(&quot;%s: sparse inference ignores n_gpu_layers, you can use --vram-budget option instead\n&quot;, __func__);
                return false;
            }
#if defined GGML_USE_CUBLAS
            llama_set_vram_budget(params.vram_budget_gb, params.main_gpu);
#endif
            llm_load_sparse_model_tensors(
                ml, model, cparams, params.main_gpu, vram_budget_bytes, params.reset_gpu_index, params.disable_gpu_index,
                params.use_mlock, params.progress_callback, params.progress_callback_user_data
            );
        } else {
            llm_load_tensors(
                ml, model, params.n_gpu_layers, params.main_gpu, params.tensor_split, params.use_mlock,
                params.progress_callback, params.progress_callback_user_data
            );
        }

    } catch (const std::exception &amp; err) {
        LLAMA_LOG_ERROR(&quot;error loading model: %s\n&quot;, err.what());
        return false;
    }

    return true;
}
</code></pre>
<p>å…¶ä¸­ï¼Œ<code>llm_load_hparams</code>ä¸­æ·»åŠ äº†å¯¹<code>sparse_pred_threshold</code>çš„è·å–å’Œè®¾ç½®ã€‚</p>
<pre><code class="language-c++">static void llm_load_hparams(
        llama_model_loader &amp; ml,
        llama_model &amp; model) {
    // -- snip --
    if (gguf_get_sparse_deriv(ctx)) {
        // read sparse threshold override if sparse deriv is enabled
        GGUF_GET_KEY(ctx, hparams.sparse_pred_threshold, gguf_get_val_f32, GGUF_TYPE_FLOAT32, false, kv(LLM_KV_SPARSE_THRESHOLD));
        if (getenv(&quot;LLAMA_SPARSE_PRED_THRESHOLD&quot;))
            hparams.sparse_pred_threshold = (float)atof(getenv(&quot;LLAMA_SPARSE_PRED_THRESHOLD&quot;));
    }
    // -- snip --
}
</code></pre>
<p>æˆ‘ä»¬æ³¨æ„åˆ°<code>llama_model_load</code>ä»£ç ä¸­ä½¿ç”¨äº†<code>ml.sparse_deriv</code>æ˜¯å¦ä¸º<code>GGML_SPARSE_INFERENCE</code>æ¥åˆ¤æ–­äº†æ˜¯å¦è¦ä½¿ç”¨PowerInferï¼ˆåŒ…æ‹¬<code>llama_use_sparse_inference</code>å‡½æ•°é‡Œé¢ä¹Ÿæ˜¯åˆ¤æ–­çš„<code>model</code>é‡Œçš„<code>sparse_deriv</code>æˆå‘˜ï¼‰ï¼Œé‚£ä¹ˆè¿™ä¸ª<code>sparse_deriv</code>æ˜¯å¦‚ä½•è¢«è¯»å–å’Œè®¾ç½®çš„å‘¢ï¼Ÿ<br>
æŸ¥è¯¢<code>gguf</code>æ–‡ä»¶çš„æ ¼å¼å¦‚ä¸‹ã€‚<br>
<img src="https://alleny.xyz/post-images/1726636250810.png" alt="" loading="lazy"><br>
æˆ‘ä»¬å¯ä»¥æ³¨æ„åˆ°ï¼Œæ–‡ä»¶çš„å‰å››ä¸ªBytesæ˜¯magic numberã€‚<br>
å†æ¥çœ‹ä»£ç ï¼Œç»“æœå°±å¾ˆæ¸…æ™°äº†ã€‚åŸæ¥PowerInferä¸ºè¿™4ä¸ªByteså¤šå¢åŠ äº†ä¸€ç§åˆæ³•çš„magic numberè§£æï¼ˆ<code>GGUF_POWERINFER_MAGIC</code>ï¼‰ï¼Œå¹¶æ ¹æ®æ­¤æ¥åˆ¤æ–­æ˜¯å¦è¯¥å¯ç”¨PowerInferçš„sparse inferenceæ¨¡å¼ã€‚</p>
<pre><code class="language-c++">struct gguf_context * gguf_init_from_file(const char * fname, struct gguf_init_params params) {
    FILE * file = fopen(fname, &quot;rb&quot;);
    if (!file) {
        return NULL;
    }

    // offset from start of file
    size_t offset = 0;

    char magic[4];
    enum ggml_sparse_deriv sparse_deriv;

    // check the magic before making allocations
    {
        gguf_fread_el(file, &amp;magic, sizeof(magic), &amp;offset);

        if (strncmp(magic, GGUF_MAGIC, sizeof(magic)) == 0) {
            sparse_deriv = GGML_DENSE_INFERENCE;
        } else if (strncmp(magic, GGUF_POWERINFER_MAGIC, sizeof(magic)) == 0) {
            sparse_deriv = GGML_SPARSE_INFERENCE;
        } else {
            fprintf(stderr, &quot;%s: invalid magic characters %s.\n&quot;, __func__, magic);
            fclose(file);
            return NULL;
        }
    }

    bool ok = true;

    struct gguf_context * ctx = GGML_ALIGNED_MALLOC(sizeof(struct gguf_context));
    ctx-&gt;sparse_deriv = sparse_deriv;

    // Other codes that read and parse the GGUF file.
    // -- snip --
}
</code></pre>
<p>å›åˆ°ä¹‹å‰çš„<code>llama_model_load</code>å‡½æ•°ã€‚å¦‚æœæ˜¯<code>GGML_SPARSE_INFERENCE</code>ï¼Œé‚£ä¹ˆPowerInferä¼šä½¿ç”¨<code>llm_load_sparse_model_tensors</code>æ¥åŠ è½½å¼ é‡ã€‚<br>
è¿™æ˜¯ä¸€ä¸ªæ¯”è¾ƒé•¿çš„å‡½æ•°ã€‚ç®€å•è€Œè¨€ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠæµç¨‹æ¦‚æ‹¬ä¸ºï¼š</p>
<ul>
<li>è®¡ç®—å†…å­˜éœ€æ±‚ï¼Œåˆ›å»ºå¹¶åˆå§‹åŒ–<code>ggml</code>ä¸Šä¸‹æ–‡ã€‚</li>
<li>æ ¹æ®æ˜¯å¦åŠ è½½äº† GPU åº“ï¼ˆå¦‚ cuBLAS æˆ– OpenCLï¼‰ï¼Œé€‰æ‹©å°†ä¸€éƒ¨åˆ†è®¡ç®—åˆ†é…ç»™ GPUã€‚</li>
<li>å‡½æ•°æ ¹æ®æ¨¡å‹çš„æ¶æ„ä¸åŒï¼Œå¯¹æ¯ç§æ¶æ„è¿›è¡Œä¸åŒçš„å¼ é‡åˆ†é…ã€‚</li>
<li>è°ƒç”¨ <code>alloc.flush()</code> æ¥å°†åˆ†é…çš„å±‚åŒæ­¥åˆ° GPUï¼ˆè®¾ç½®backendä»¥åŠå‡budgetï¼‰ã€‚</li>
<li>é€šè¿‡<code>ml.load_all_data</code>å°†å°†æ¨¡å‹çš„æ‰€æœ‰æ•°æ®åŠ è½½åˆ°å†…å­˜ä¸­ã€‚</li>
<li>å¦‚æœ cparams å­˜åœ¨ï¼Œåˆ†é… KV Cacheï¼ˆçš„ç©ºé—´ï¼‰ã€‚</li>
<li>è°ƒç”¨ <code>llm_load_gpu_split</code> æ¥å¤„ç† FFN ç½‘ç»œçš„åˆ†ç‰‡å’Œåˆ†é…ã€‚</li>
</ul>
<pre><code class="language-c++">static void llm_load_sparse_model_tensors(
        llama_model_loader &amp; ml,
        llama_model &amp; model,
        const llama_context_params * cparams,
        int main_gpu,
        long int vram_budget_bytes,
        bool reset_gpu_index,
        bool disable_ffn_split,
        bool use_mlock,
        llama_progress_callback progress_callback,
        void * progress_callback_user_data) {
    model.t_start_us = ggml_time_us();
    auto &amp; ctx     = model.ctx;
    auto &amp; hparams = model.hparams;

    size_t ctx_size;
    size_t mmapped_size;
    ml.calc_sizes(ctx_size, mmapped_size);
    LLAMA_LOG_INFO(&quot;%s: ggml ctx size = %7.2f MB\n&quot;, __func__, ctx_size/1024.0/1024.0);

    // create the ggml context
    {
        model.buf.resize(ctx_size);
        if (use_mlock) {
            model.mlock_buf.init   (model.buf.data);
            model.mlock_buf.grow_to(model.buf.size);
        }

        struct ggml_init_params params = {
            /*.mem_size   =*/ model.buf.size,
            /*.mem_buffer =*/ model.buf.data,
            /*.no_alloc   =*/ ml.use_mmap,
        };

        model.ctx = ggml_init(params);
        if (!model.ctx) {
            throw std::runtime_error(format(&quot;ggml_init() failed&quot;));
        }
    }

    (void) main_gpu;

    enum ggml_backend_type llama_backend_offload = GGML_BACKEND_CPU;
    enum ggml_backend_type llama_backend_offload_split = GGML_BACKEND_CPU;

#ifdef GGML_USE_CUBLAS
    if (ggml_cublas_loaded()) {
        LLAMA_LOG_INFO(&quot;%s: using &quot; GGML_CUDA_NAME &quot; for GPU acceleration\n&quot;, __func__);
        ggml_cuda_set_main_device(main_gpu);

        llama_backend_offload = GGML_BACKEND_GPU;
        llama_backend_offload_split = GGML_BACKEND_GPU_SPLIT;
    }
#elif defined(GGML_USE_CLBLAST)
        LLAMA_LOG_INFO(&quot;%s: using OpenCL for GPU acceleration\n&quot;, __func__);
        llama_backend_offload = GGML_BACKEND_GPU;
        llama_backend_offload_split = GGML_BACKEND_GPU;
#endif

    buffered_tensor_allocator alloc(ml, ctx, hparams);
    uint32_t current_layer = 0;
    auto create_tensor = [&amp;alloc, &amp;current_layer] (
        const std::pair&lt;std::string, llm_tensor&gt; &amp; tn, 
        const std::vector&lt;int64_t&gt; &amp; ne) -&gt; ggml_tensor * {
        return alloc.buffered_alloc(tn.first, tn.second, ne, current_layer);
    };

    {
        const int64_t n_embd     = hparams.n_embd;
        const int64_t n_embd_gqa = hparams.n_embd_gqa();
        const int64_t n_layer    = hparams.n_layer;
        const int64_t n_vocab    = hparams.n_vocab;

        const auto tn = LLM_TN(model.arch);
        switch (model.arch) {
            case LLM_ARCH_LLAMA:
            case LLM_ARCH_REFACT:
            case LLM_ARCH_BAMBOO:
                {
                    model.tok_embd = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, &quot;weight&quot;), {n_embd, n_vocab});

                    // output
                    {
                        model.output_norm = create_tensor(tn(LLM_TENSOR_OUTPUT_NORM, &quot;weight&quot;), {n_embd});
                        model.output      = create_tensor(tn(LLM_TENSOR_OUTPUT,      &quot;weight&quot;), {n_embd, n_vocab});
                    }

                    const uint32_t n_ff = hparams.n_ff;
                    model.layers.resize(n_layer);

                    for (uint32_t &amp;i = current_layer; i &lt; n_layer; ++i) {
                       auto &amp; layer = model.layers[i];

                        layer.attn_norm = create_tensor(tn(LLM_TENSOR_ATTN_NORM, &quot;weight&quot;, i), {n_embd});

                        layer.wq = create_tensor(tn(LLM_TENSOR_ATTN_Q,   &quot;weight&quot;, i), {n_embd, n_embd});
                        layer.wk = create_tensor(tn(LLM_TENSOR_ATTN_K,   &quot;weight&quot;, i), {n_embd, n_embd_gqa});
                        layer.wv = create_tensor(tn(LLM_TENSOR_ATTN_V,   &quot;weight&quot;, i), {n_embd, n_embd_gqa});
                        layer.wo = create_tensor(tn(LLM_TENSOR_ATTN_OUT, &quot;weight&quot;, i), {n_embd, n_embd});

                        layer.ffn_norm = create_tensor(tn(LLM_TENSOR_FFN_NORM, &quot;weight&quot;, i), {n_embd});

                        layer.ffn_gate = create_tensor(tn(LLM_TENSOR_FFN_GATE, &quot;weight&quot;, i), {n_embd,   n_ff});
                        layer.ffn_down_t = create_tensor(tn(LLM_TENSOR_FFN_DOWN_T, &quot;weight&quot;, i), {n_embd, n_ff});
                        layer.mlp_pre_w1 = create_tensor(tn(LLM_TENSOR_MLP_PRED_FC1, &quot;weight&quot;, i), {n_embd, GGML_NE_WILDCARD});
                        layer.mlp_pre_w2 = create_tensor(tn(LLM_TENSOR_MLP_PRED_FC2, &quot;weight&quot;, i), {GGML_NE_WILDCARD, n_ff});
                        layer.ffn_up   = create_tensor(tn(LLM_TENSOR_FFN_UP,   &quot;weight&quot;, i), {n_embd,   n_ff});
                    }
                } break;
            case LLM_ARCH_FALCON:
                {
                    model.tok_embd = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, &quot;weight&quot;), {n_embd, n_vocab});

                    // output
                    {
                        model.output_norm   = create_tensor(tn(LLM_TENSOR_OUTPUT_NORM, &quot;weight&quot;), {n_embd});
                        model.output_norm_b = create_tensor(tn(LLM_TENSOR_OUTPUT_NORM, &quot;bias&quot;),   {n_embd});
                        model.output        = create_tensor(tn(LLM_TENSOR_OUTPUT,      &quot;weight&quot;), {n_embd, n_vocab});
                    }

                    const uint32_t n_ff = hparams.n_ff;

                    model.layers.resize(n_layer);

                    for (uint32_t &amp;i = current_layer; i &lt; n_layer; ++i) {
                        auto &amp; layer = model.layers[i];

                        layer.attn_norm   = create_tensor(tn(LLM_TENSOR_ATTN_NORM,   &quot;weight&quot;, i), {n_embd});
                        layer.attn_norm_b = create_tensor(tn(LLM_TENSOR_ATTN_NORM,   &quot;bias&quot;, i),   {n_embd});

                        if (gguf_find_tensor(ml.ctx_gguf, tn(LLM_TENSOR_ATTN_NORM_2, &quot;weight&quot;, i).first.c_str()) &gt;= 0) {
                            layer.attn_norm_2   = create_tensor(tn(LLM_TENSOR_ATTN_NORM_2, &quot;weight&quot;, i), {n_embd});
                            layer.attn_norm_2_b = create_tensor(tn(LLM_TENSOR_ATTN_NORM_2, &quot;bias&quot;, i),   {n_embd});
                        }

                        layer.wqkv = create_tensor(tn(LLM_TENSOR_ATTN_QKV, &quot;weight&quot;, i), {n_embd, n_embd + 2*n_embd_gqa});
                        layer.wo   = create_tensor(tn(LLM_TENSOR_ATTN_OUT, &quot;weight&quot;, i), {n_embd, n_embd});
                        layer.ffn_down_t = create_tensor(tn(LLM_TENSOR_FFN_DOWN_T, &quot;weight&quot;, i), {n_embd, n_ff});
                        layer.mlp_pre_w1 = create_tensor(tn(LLM_TENSOR_MLP_PRED_FC1, &quot;weight&quot;, i), {n_embd, GGML_NE_WILDCARD});
                        layer.mlp_pre_w2 = create_tensor(tn(LLM_TENSOR_MLP_PRED_FC2, &quot;weight&quot;, i), {GGML_NE_WILDCARD, n_ff});
                        layer.ffn_up   = create_tensor(tn(LLM_TENSOR_FFN_UP,   &quot;weight&quot;, i), {n_embd,   n_ff});
                    }
                } break;
            default:
                throw std::runtime_error(&quot;unknown architecture&quot;);
        }
    }

    model.n_gpu_layers = alloc.flush();
    LLAMA_LOG_INFO(&quot;%s: offloaded layers from VRAM budget(%ld bytes): %d/%d\n&quot;, __func__, vram_budget_bytes, model.n_gpu_layers, hparams.n_layer);

    // print memory requirements
    {
        // this is the total memory required to run the inference
        size_t mem_required = ctx_size + mmapped_size;

        LLAMA_LOG_INFO(&quot;%s: mem required  = %7.2f MB\n&quot;, __func__, mem_required / 1024.0 / 1024.0);

#if defined(GGML_USE_CUBLAS) || defined(GGML_USE_CLBLAST)
        LLAMA_LOG_INFO(&quot;%s: VRAM used: %.2f MB\n&quot;, __func__, alloc.vram_allocated_bytes / 1024.0 / 1024.0);
#endif
    }

    // populate `tensors_by_name`
    for (int i = 0; i &lt; ml.n_tensors; ++i) {
        struct ggml_tensor * cur = ggml_get_tensor(ctx, ml.get_tensor_name(i));
        model.tensors_by_name.emplace_back(ggml_get_name(cur), cur);
    }

    ml.load_all_data(ctx, progress_callback, progress_callback_user_data, use_mlock ? &amp;model.mlock_mmap : NULL);

    if (progress_callback) {
        progress_callback(1.0f, progress_callback_user_data);
    }

    model.mapping = std::move(ml.mapping);

    // Reserve KV cache in VRAM
    if (cparams != NULL) {
        llama_reserve_model_kv_cache(&amp;model, cparams);
    }
    // Offload FFN segments to GPU if possible
    model.ffn_offloaded_bytes = llm_load_gpu_split(ml, model, reset_gpu_index, disable_ffn_split || !alloc.tensor_offload_complete);

    // loading time will be recalculate after the first eval, so
    // we take page faults deferred by mmap() into consideration
    model.t_load_us = ggml_time_us() - model.t_start_us;
}
</code></pre>
<p>é‡ç‚¹çœ‹ä¸€ä¸‹æœ€åçš„<code>llm_load_gpu_split</code>ã€‚å…¶ä¸»è¦è°ƒç”¨äº†<code>llm_load_gpu_split_with_budget</code>å’Œ<code>llama_model_offload_ffn_split</code>ã€‚å…¶ä¸­<code>llm_load_gpu_split_with_budget</code>ä¸»è¦è´Ÿè´£åŠ è½½GPU Indexï¼ˆå³å“ªäº›è®¡ç®—è¢«å¸è½½åˆ°GPUä¸Šï¼‰ï¼Œ<code>llama_model_offload_ffn_split</code>åˆ™è´Ÿè´£æ ¹æ®GPU IndexæŠŠæ¯å±‚çš„Feed Forwardå±‚æŒ‰éœ€åŠ è½½åˆ°GPUä¸Šã€‚</p>
<pre><code class="language-c++">static size_t llm_load_gpu_split(llama_model_loader &amp; ml, llama_model &amp; model, bool no_cache, bool no_offload) {
#if defined (GGML_USE_CUBLAS)
    if (!ggml_cublas_loaded()) {
        throw std::runtime_error(format(&quot;cannot offload to GPU: &quot; GGML_CUDA_NAME &quot; not loaded&quot;));
    }
    if (!no_offload &amp;&amp; !llm_load_gpu_split_with_budget(ml, model, vram_budget_bytes, no_cache)) {
        LLAMA_LOG_ERROR(&quot;%s: error: failed to generate gpu split, an empty one will be used\n&quot;, __func__);
    }
#endif

    // Apply GPU index and split FFNs to GPU
    size_t ffn_offloaded_bytes = llama_model_offload_ffn_split(&amp;model);
    LLAMA_LOG_INFO(&quot;%s: offloaded %.2f MiB of FFN weights to GPU\n&quot;, __func__, ffn_offloaded_bytes / 1024.0 / 1024.0);

    return ffn_offloaded_bytes;
}
</code></pre>
<p><code>llm_load_gpu_split_with_budget</code>ä¼šå°è¯•åŠ è½½<code>ml.file.fname + &quot;.generated.gpuidx&quot;</code>æ–‡ä»¶ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è°ƒç”¨<code>python</code>è„šæœ¬ç”Ÿæˆä¸€ä¸ªè¯¥æ–‡ä»¶ã€‚è„šæœ¬çš„ä¸»è¦ä½œç”¨æ˜¯é€šè¿‡æ•´æ•°çº¿æ€§è§„åˆ’ï¼ˆå…·ä½“ç®—æ³•å¯è§è®ºæ–‡ï¼‰ï¼Œå¾—å‡ºåº”è¯¥æŠŠå“ªäº›è®¡ç®—å¸è½½åˆ°GPUä¸Šã€‚å…¶ä¸­ï¼Œè„šæœ¬ä¼šå¾—åˆ°<code>gpu_idx</code>å’Œ<code>gpu_bucket</code>ã€‚</p>
<ul>
<li><code>gpu_idx</code> æ˜¯ä¸€ä¸ªå¸ƒå°”å‹ï¼ˆ0/1ï¼‰çš„å¼ é‡ï¼Œç”¨äºæŒ‡ç¤ºå“ªäº›ç¥ç»å…ƒï¼ˆæˆ–å¼ é‡çš„éƒ¨åˆ†ï¼‰ä¼šè¢«åˆ†é…åˆ° GPU ä¸Šã€‚å®ƒçš„æ¯ä¸ªå…ƒç´ ä¸å¯¹åº”å±‚çš„æ¿€æ´»æ•°æ®ï¼ˆç¥ç»å…ƒï¼‰ä¸€ä¸€å¯¹åº”ã€‚å¦‚æœæŸä¸ªä½ç½®çš„å€¼ä¸º 1ï¼Œåˆ™è¡¨ç¤ºè¯¥ç¥ç»å…ƒéœ€è¦åˆ†é…åˆ° GPUï¼›å¦‚æœå€¼ä¸º 0ï¼Œåˆ™è¡¨ç¤ºè¯¥ç¥ç»å…ƒä¸åœ¨ GPU ä¸Šã€‚</li>
<li><code>gpu_bucket</code> æ˜¯ä¸€ä¸ªè¡¨ç¤ºåœ¨ GPU ä¸Šå®é™…å­˜å‚¨çš„ç¥ç»å…ƒç´¢å¼•çš„æœ‰åºåˆ—è¡¨ï¼ˆæ•°ç»„ï¼‰ã€‚å®ƒä»…åŒ…å«é‚£äº›è¢«é€‰ä¸­è¦æ”¾ç½®åˆ° GPU ä¸Šçš„ç¥ç»å…ƒç´¢å¼•ï¼Œå¹¶ä¸”é€šå¸¸æ˜¯æœ‰åºçš„ã€‚è¿™ç§æ–¹å¼ä¾¿äºåœ¨ GPU å†…å­˜ä¸­é«˜æ•ˆå­˜å‚¨å’ŒæŸ¥æ‰¾ã€‚<br>
æœ€åï¼Œé€šè¿‡å¯¹<code>.gpuidx</code>æ–‡ä»¶çš„åŠ è½½ï¼Œ<code>gpu_idx</code>å’Œ<code>gpu_bucket</code>ä¿¡æ¯ä¼šè¢«åŠ è½½åˆ°modelçš„layerä¸­ï¼ˆé¡ºä¾¿è®¡ç®—äº†ä¸ª<code>model_layer.gpu_offload_ratio</code>ï¼‰ã€‚</li>
</ul>
<pre><code class="language-c++">    int load_gpu_idx_for_model(llama_model * model) {
        int n_layers = model-&gt;layers.size();
        // TODO: assert fp is at the end of headers
        if (n_tensors != n_layers * 2) {
           LLAMA_LOG_ERROR(&quot;%s: error: the number of gpu splits does not match the layer of model\n&quot;, __func__);
            return 1;
        }
        LLAMA_LOG_INFO(&quot;%s: applying gpu_idx adapter from '%s' - please wait ...\n&quot;, __func__, fname.c_str());
        const int64_t t_start_mlp_us = ggml_time_us();

        for (int il = 0; il &lt; n_layers; il++) {
            llama_layer &amp;model_layer = model-&gt;layers[il];
            ggml_tensor * gpu_idx = idx_loader-&gt;get_tensor_meta(il*2);
            ggml_tensor * gpu_bucket = idx_loader-&gt;get_tensor_meta(il*2+1);
            if (gpu_idx == nullptr || gpu_bucket == nullptr) {
                LLAMA_LOG_ERROR(&quot;%s: error: failed to load gpu index or bucket\n&quot;, __func__);
                return 1;
            }
            model_layer.gpu_idx = idx_loader-&gt;create_tensor_for(ctx_meta, gpu_idx, GGML_BACKEND_CPU);
            model_layer.gpu_bucket = idx_loader-&gt;create_tensor_for(ctx_meta, gpu_bucket, GGML_BACKEND_CPU);
        }
        llama_progress_callback cb = [](float progress, void *ctx) {
            LLAMA_LOG_INFO(&quot;.&quot;);
        };
        idx_loader-&gt;load_all_data(ctx_meta, cb, nullptr, nullptr);

        for (int il = 0; il &lt; n_layers; il++) {
            llama_layer &amp;model_layer = model-&gt;layers[il];
            ggml_tensor * gpu_idx = model_layer.gpu_idx;
            ggml_tensor * gpu_bucket = model_layer.gpu_bucket;
            int64_t gpu_neurons = sum_gpu_index(gpu_idx);
            model_layer.gpu_offload_ratio = (double)gpu_neurons / gpu_idx-&gt;ne[0];
            if (gpu_neurons == 0 || gpu_neurons == gpu_idx-&gt;ne[0]) {
                // no hybrid inference for this layer, unset gpu_bucket
                model_layer.gpu_bucket = NULL;
                // TODO: maybe can also unset gpu_idx
            } else {
#if defined(GGML_USE_CUBLAS)
                ggml_set_backend(gpu_bucket, GGML_BACKEND_GPU);
                ggml_cuda_transform_tensor(gpu_bucket-&gt;data, gpu_bucket);
#else
                GGML_ASSERT(false &amp;&amp; &quot;cublas is not enabled&quot;);
#endif
            }
        }

        const int64_t t_mlp_us = ggml_time_us() - t_start_mlp_us;
        LLAMA_LOG_INFO(&quot; done (%.2f ms)\n&quot;, t_mlp_us / 1000.0);

        return 0;
    }
</code></pre>
<p>è‡³äº<code>llama_model_offload_ffn_split</code>ï¼Œæœ€ç»ˆä¼šå¯¹æ¯ä¸€å±‚è°ƒç”¨<code>slice_ffn_mat_to_gpu</code>æ¥æ ¹æ®<code>gpu_bucket</code>æŠŠ<code>ffn_gate</code>ã€<code>ffn_up</code>å’Œ<code>ffn_down</code>å¸è½½åˆ°GPUä¸Šã€‚</p>
<pre><code class="language-c++">   size_t slice_ffn_mat_to_gpu(llama_layer &amp; layer) {
        std::vector&lt;uint8_t&gt; work_buffer;
        ggml_tensor * gpu_idx = layer.gpu_idx;
        ggml_tensor * gpu_bucket = layer.gpu_bucket;
        size_t offloaded_bytes = 0;

        if (layer.gpu_offload_ratio == 0.) {
            return 0;
        }

        GGML_ASSERT((layer.gpu_bucket != NULL) == (layer.gpu_offload_ratio &lt; 1.0));

        if (layer.ffn_gate) {
            layer.ffn_gate_gpu = create_striped_mat_to_gpu(layer.ffn_gate, gpu_bucket);
            offloaded_bytes += ggml_nbytes(layer.ffn_gate_gpu);
        }
        
        layer.ffn_up_gpu = create_striped_mat_to_gpu(layer.ffn_up, gpu_bucket);
        offloaded_bytes += ggml_nbytes(layer.ffn_up_gpu);
        
        layer.ffn_down_gpu = create_striped_mat_to_gpu(layer.ffn_down_t, gpu_bucket);
        offloaded_bytes += ggml_nbytes(layer.ffn_down_gpu);

        return offloaded_bytes;
    }

    size_t offload_ffn_split(llama_model * model) {
        LLAMA_LOG_INFO(&quot;%s: applying augmentation to model - please wait ...\n&quot;, __func__);
        const int64_t t_start_aug_us = ggml_time_us();
        std::vector&lt;uint8_t&gt; work_buffer;

        // Set sparsity threshold via global virables
        sparse_pred_threshold = model-&gt;hparams.sparse_pred_threshold;
#if defined (GGML_USE_CUBLAS)
        ggml_cuda_set_device_constants(model-&gt;hparams.sparse_pred_threshold);
#endif

        // load gpu_idx and slice mat to gpu
        size_t offloaded_bytes = 0;
        for (llama_layer &amp;model_layer : model -&gt; layers) {
            // gpu_idx load
            if (model_layer.gpu_idx == NULL &amp;&amp; model_layer.gpu_bucket == NULL) {
                ggml_tensor * gpu_idx = ggml_new_tensor_1d(aux_ctx, GGML_TYPE_I32, model_layer.mlp_pre_w2 -&gt; ne[1]);
                ggml_set_zero(gpu_idx);
                model_layer.gpu_idx = gpu_idx;
                ggml_tensor * gpu_bucket = ggml_new_tensor_1d(aux_ctx, GGML_TYPE_I32, 0);
                model_layer.gpu_bucket = gpu_bucket;
            }
            offloaded_bytes += slice_ffn_mat_to_gpu(model_layer);
            LLAMA_LOG_INFO(&quot;.&quot;);
        }

        LLAMA_LOG_INFO(&quot; done (%.2f ms)\n&quot;, (ggml_time_us() - t_start_aug_us) / 1000.0);
        return offloaded_bytes;
    }
</code></pre>
<p>ç‚¹è¿›<code>create_striped_mat_to_gpu</code>ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ°weightsæ˜¯å¦‚ä½•æ ¹æ®<code>gpu_bucket</code>è¢«å¸è½½åˆ°GPUä¸Šçš„ã€‚ç®€å•è¯´ï¼Œ<code>gpu_bucket</code>ä¸­å­˜å‚¨çš„æ˜¯<code>host_i</code>ï¼Œæ®æ­¤æˆ‘ä»¬æ‰¾åˆ°hostä¸­çš„<code>(char *)(src -&gt; data) + host_i * row_data_size</code>ä½ç½®æ¥å°†æ•°æ®ä¸€è¡Œä¸€è¡Œcopyåˆ°GPUä¸Šã€‚</p>
<pre><code class="language-c++">    ggml_tensor * create_striped_mat_to_gpu(struct ggml_tensor *src, struct ggml_tensor * gpu_bucket) {
#ifdef GGML_USE_CUBLAS
        if (gpu_bucket == NULL) {
            // offload the whole tensor to gpu
            ggml_set_backend(src, GGML_BACKEND_GPU);
            ggml_cuda_transform_tensor(src-&gt;data, src);
            return src;
        }

        int64_t row_len = src-&gt;ne[0];
        int64_t gpu_rows = gpu_bucket-&gt;ne[0];
        GGML_ASSERT(0 &lt; gpu_rows &amp;&amp; gpu_rows &lt;= src-&gt;ne[1]);

        ggml_set_no_alloc(aux_ctx, true);
        ggml_tensor * gpu_dst = ggml_new_tensor_2d(aux_ctx, src-&gt;type, row_len, gpu_rows);
        ggml_set_backend(gpu_dst, GGML_BACKEND_GPU);
        ggml_cuda_alloc_tensor(gpu_dst);

        // init two 1d views on host and device
        ggml_tensor * host_mat_row = ggml_new_tensor_1d(aux_ctx, src-&gt;type, row_len);
        static ggml_tensor * device_mat_row = ggml_dup_tensor(aux_ctx, host_mat_row);
        ggml_set_backend(device_mat_row, GGML_BACKEND_GPU);
        ggml_cuda_alloc_tensor(device_mat_row);
        *ggml_cuda_get_data_pp(device_mat_row) = *ggml_cuda_get_data_pp(gpu_dst);

        // read raw data and copy to device depending on gpu_idx
        const enum ggml_type type = src-&gt;type;
        const int ne0 = src-&gt;ne[0];
        const size_t row_data_size = ne0*ggml_type_size(type)/ggml_blck_size(type);
        for (int i = 0; i &lt; gpu_rows; i++) {
            int32_t host_i = ((int32_t *)gpu_bucket-&gt;data)[i];
            host_mat_row -&gt; data = (char *)(src -&gt; data) + host_i * row_data_size;
            char ** gpu_data_pp = reinterpret_cast&lt;char **&gt;(ggml_cuda_get_data_pp(device_mat_row));
            // printf(&quot;gpu_data_p: %p\n&quot;, *gpu_data_pp);
            ggml_cuda_cpy_1d(device_mat_row, host_mat_row);
            *gpu_data_pp = *gpu_data_pp + row_data_size;
        }
        ggml_set_no_alloc(aux_ctx, false);

        return gpu_dst;
#else
        return NULL;
#endif
    }
</code></pre>
<p>åˆ°è¿™é‡Œï¼Œæ¨¡å‹tensorè¢«åˆ›å»ºï¼Œæ•°æ®è¢«åŠ è½½ï¼ŒGPU Indexè¢«è®¡ç®—ï¼ŒGPUä¸Šä¹Ÿè¢«åˆ†é…å’Œå¸è½½äº†ç›¸åº”å†…å®¹ï¼Œæ¨¡å‹çš„åˆå§‹åŠ è½½è¿‡ç¨‹ä¾¿å¤§è‡´ç»“æŸã€‚</p>
<h2 id="æ”¯æŒ-ï¸">æ”¯æŒ â˜•ï¸</h2>
<p>å¦‚æœå‘ç°å†…å®¹æœ‰çº°æ¼æˆ–é”™è¯¯ï¼Œå¯ä»¥é€šè¿‡é‚®ç®±hangyu.yuan@qq.comè”ç³»æˆ‘æˆ–ç›´æ¥åœ¨ä¸‹æ–¹è¯„è®ºå‘Šè¯‰æˆ‘ï¼Œè°¢è°¢ã€‚<br>
æˆ‘çš„<a href="https://github.com/Yuan-Allen">GitHubä¸»é¡µ</a>ã€‚</p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#%E7%9B%B8%E5%85%B3">ç›¸å…³</a></li>
<li><a href="#%E8%83%8C%E6%99%AF">èƒŒæ™¯</a></li>
<li><a href="#%E8%AF%B4%E6%98%8E">è¯´æ˜</a></li>
<li><a href="#ggml">GGML</a></li>
<li><a href="#%E6%AD%A3%E6%96%87">æ­£æ–‡</a></li>
<li><a href="#%E6%94%AF%E6%8C%81-%EF%B8%8F">æ”¯æŒ â˜•ï¸</a></li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">ä¸‹ä¸€ç¯‡</div>
            <a href="https://alleny.xyz/post/leetcode-stack-and-queue/">
              <h3 class="post-title">
                LeetCodeåˆ·é¢˜è®°å½•ï¼ˆæ ˆä¸é˜Ÿåˆ—ç¯‡ï¼‰
              </h3>
            </a>
          </div>
        

        
          

          
            <link rel="stylesheet" href="https://unpkg.com/disqusjs@1.1/dist/disqusjs.css">
<script src="https://unpkg.com/disqusjs@1.1/dist/disqus.js"></script>

<div id="disqus_thread"></div>

<script>

var options = {
  shortname: 'alleny-blog',
  apikey: '2kPJh2tQ0rWB7n0QOhvb9TLbm946tHMWCixW2qGF9j8tfMBgZoNPfvLpDoxZTZpD',
}
if ('https://disqus.hangyu-yuan.workers.dev/api/') {
  options.api = 'https://disqus.hangyu-yuan.workers.dev/api/'
}
var dsqjs = new DisqusJS(options)

</script>

          
        

        <div class="site-footer">
  Copyright Â© 2023-2025 Allen Yuan. All rights reserved.
  <a class="rss" href="https://alleny.xyz/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
