<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>PowerInferæºç è§£æï¼ˆä¸‰ï¼‰ï¼šç®—å­å®ç° | AllenY&#39;s blog</title>
<link rel="shortcut icon" href="https://alleny.xyz/favicon.ico?v=1747120786321">
<link href="https://cdn.jsdelivr.net/npm/remixicon@2.3.0/fonts/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://alleny.xyz/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="PowerInferæºç è§£æï¼ˆä¸‰ï¼‰ï¼šç®—å­å®ç° | AllenY&#39;s blog - Atom Feed" href="https://alleny.xyz/atom.xml">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">


<script async src="https://www.googletagmanager.com/gtag/js?id=G-HPLP8D1S43"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-HPLP8D1S43');
</script>


    <meta name="description" content="è¿™æ¬¡æˆ‘ä»¬æ¥è§£æä¸€ä¸‹PowerInferä¸»è¦æ‰€ä½¿ç”¨åˆ°çš„ç®—å­æ˜¯å¦‚ä½•å®ç°çš„ã€‚

ç›¸å…³

PowerInferæºç è§£æï¼ˆä¸€ï¼‰ï¼šæ¨¡å‹åŠ è½½ã€‚
PowerInferæºç è§£æï¼ˆäºŒï¼‰ï¼šè®¡ç®—å›¾æ„å»ºä¸ç®—å­è°ƒç”¨ ã€‚

ggml_compute_forward_m..." />
    <meta name="keywords" content="System for AI,Technical Sharing" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
    <script src="//cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.5.1/build/highlight.min.js"></script>
  </head>
  <body>
    <div class="main">
      <div class="main-content">
        <div class="site-header">
  <a href="https://alleny.xyz">
  <img class="avatar" src="https://alleny.xyz/images/avatar.png?v=1747120786321" alt="">
  </a>
  <h1 class="site-title">
    AllenY&#39;s blog
  </h1>
  <p class="site-description">
    ğŸ§‘ğŸ»â€ğŸ’»ğŸ®ğŸ¿ğŸ¹ğŸ—»
  </p>
  <div class="menu-container">
    
      
        <a href="/" class="menu">
          Home
        </a>
      
    
      
        <a href="/archives" class="menu">
          Archive
        </a>
      
    
      
        <a href="/tags" class="menu">
          Tags
        </a>
      
    
      
        <a href="/post/about" class="menu">
          About
        </a>
      
    
  </div>
  <div class="social-container">
    
      
        <a href="https://github.com/Yuan-Allen" target="_blank">
          <i class="ri-github-line"></i>
        </a>
      
    
      
    
      
        <a href="https://weibo.com/u/6478080851" target="_blank">
          <i class="ri-weibo-line"></i>
        </a>
      
    
      
    
      
    
  </div>
</div>

        <div class="post-detail">
          <article class="post">
            <h2 class="post-title">
              PowerInferæºç è§£æï¼ˆä¸‰ï¼‰ï¼šç®—å­å®ç°
            </h2>
            <div class="post-info">
              <span>
                2024-09-29
              </span>
              <span>
                32 min read
              </span>
              
                <a href="https://alleny.xyz/tag/system-for-ai/" class="post-tag">
                  # System for AI
                </a>
              
                <a href="https://alleny.xyz/tag/WT-UATSMl/" class="post-tag">
                  # Technical Sharing
                </a>
              
            </div>
            
              <img class="post-feature-image" src="https://alleny.xyz/post-images/powerinfer-source-analysis-3.png" alt="">
            
            <div class="post-content-wrapper">
              <div class="post-content" v-pre>
                <p>è¿™æ¬¡æˆ‘ä»¬æ¥è§£æä¸€ä¸‹PowerInferä¸»è¦æ‰€ä½¿ç”¨åˆ°çš„ç®—å­æ˜¯å¦‚ä½•å®ç°çš„ã€‚</p>
<!-- more -->
<h2 id="ç›¸å…³">ç›¸å…³</h2>
<ul>
<li><a href="https://alleny.xyz/post/powerinfer-source-analysis-1/">PowerInferæºç è§£æï¼ˆä¸€ï¼‰ï¼šæ¨¡å‹åŠ è½½</a>ã€‚</li>
<li><a href="https://alleny.xyz/post/powerinfer-source-analysis-2/">PowerInferæºç è§£æï¼ˆäºŒï¼‰ï¼šè®¡ç®—å›¾æ„å»ºä¸ç®—å­è°ƒç”¨ </a>ã€‚</li>
</ul>
<h2 id="ggml_compute_forward_mul_mat">ggml_compute_forward_mul_mat</h2>
<p>æˆ‘ä»¬å…ˆæ¥çœ‹åŸæœ¬çŸ©é˜µä¹˜æ‰€é»˜è®¤ä½¿ç”¨ç®—å­ï¼ˆCPUç‰ˆæœ¬ï¼‰<code>ggml_compute_forward_mul_mat</code>ã€‚</p>
<pre><code class="language-c++">static void ggml_compute_forward_mul_mat(
        const struct ggml_compute_params * params,
        const struct ggml_tensor * src0,
        const struct ggml_tensor * src1,
              struct ggml_tensor * dst) {
    int64_t t0 = ggml_perf_time_us();
    UNUSED(t0);

    GGML_TENSOR_BINARY_OP_LOCALS

    const int ith = params-&gt;ith;
    const int nth = params-&gt;nth;

    const enum ggml_type type = src0-&gt;type;

    const bool src1_cont = ggml_is_contiguous(src1);

    ggml_vec_dot_t    const vec_dot               = type_traits[type].vec_dot;
    enum ggml_type    const vec_dot_type          = type_traits[type].vec_dot_type;
    ggml_from_float_t const from_float_to_vec_dot = type_traits[vec_dot_type].from_float;

    GGML_ASSERT(ne0 == ne01);
    GGML_ASSERT(ne1 == ne11);
    GGML_ASSERT(ne2 == ne12);
    GGML_ASSERT(ne3 == ne13);

    // we don't support permuted src0 or src1
    GGML_ASSERT(nb00 == ggml_type_size(type));
    GGML_ASSERT(nb10 == ggml_type_size(src1-&gt;type));

    // dst cannot be transposed or permuted
    GGML_ASSERT(nb0 == sizeof(float));
    GGML_ASSERT(nb0 &lt;= nb1);
    GGML_ASSERT(nb1 &lt;= nb2);
    GGML_ASSERT(nb2 &lt;= nb3);

    // broadcast factors
    const int64_t r2 = ne12/ne02;
    const int64_t r3 = ne13/ne03;

    // nb01 &gt;= nb00 - src0 is not transposed
    //   compute by src0 rows

#if defined(GGML_USE_CLBLAST)
    if (ggml_cl_can_mul_mat(src0, src1, dst)) {
        if (params-&gt;ith == 0 &amp;&amp; params-&gt;type == GGML_TASK_COMPUTE) {
            ggml_cl_mul_mat(src0, src1, dst, params-&gt;wdata, params-&gt;wsize);
        }
        return;
    }
#endif

#if defined(GGML_USE_ACCELERATE) || defined(GGML_USE_OPENBLAS)
    if (ggml_compute_forward_mul_mat_use_blas(src0, src1, dst)) {
        if (params-&gt;ith != 0) {
            return;
        }

        if (params-&gt;type == GGML_TASK_INIT) {
            return;
        }

        if (params-&gt;type == GGML_TASK_FINALIZE) {
            return;
        }

        for (int64_t i13 = 0; i13 &lt; ne13; i13++) {
            for (int64_t i12 = 0; i12 &lt; ne12; i12++) {
                // broadcast src0 into src1 across 2nd,3rd dimension
                const int64_t i03 = i13/r3;
                const int64_t i02 = i12/r2;

                const void  * x = (char *)            src0-&gt;data + i02*nb02 + i03*nb03;
                const float * y = (float *) ((char *) src1-&gt;data + i12*nb12 + i13*nb13);

                float * d = (float *) ((char *) dst-&gt;data + i12*nb2 + i13*nb3);

                if (type != GGML_TYPE_F32) {
                            float * const wdata    = params-&gt;wdata;
                    ggml_to_float_t const to_float = type_traits[type].to_float;

                    size_t id = 0;
                    for (int64_t i01 = 0; i01 &lt; ne01; ++i01) {
                        to_float((const char *) x + i01*nb01, wdata + id, ne00);
                        id += ne00;
                    }

                    assert(id*sizeof(float) &lt;= params-&gt;wsize);
                    x = wdata;
                }

                cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasTrans,
                        ne11, ne01, ne10,
                        1.0f,    y, ne10,
                                 x, ne00,
                        0.0f,    d, ne01);
            }
        }

        //printf(&quot;CBLAS = %f ms, %d x %d x %d x %d\n&quot;, (ggml_perf_time_us() - t0)/1000.0, ne0, ne1, ne2, ne3);

        return;
    }
#endif

    if (params-&gt;type == GGML_TASK_INIT) {
        if (src1-&gt;type != vec_dot_type) {
            char * wdata = params-&gt;wdata;
            const size_t row_size = ne10*ggml_type_size(vec_dot_type)/ggml_blck_size(vec_dot_type);

            for (int64_t i13 = 0; i13 &lt; ne13; ++i13) {
                for (int64_t i12 = 0; i12 &lt; ne12; ++i12) {
                    for (int64_t i11 = 0; i11 &lt; ne11; ++i11) {
                        from_float_to_vec_dot((float *)((char *) src1-&gt;data + i13*nb13 + i12*nb12 + i11*nb11), (void *) wdata, ne10);
                        wdata += row_size;
                    }
                }
            }
        }

        return;
    }

    if (params-&gt;type == GGML_TASK_FINALIZE) {
        return;
    }

    const void * wdata    = (src1-&gt;type == vec_dot_type) ? src1-&gt;data : params-&gt;wdata;
    const size_t row_size = ne10*ggml_type_size(vec_dot_type)/ggml_blck_size(vec_dot_type);

    const int64_t nr0 = ne01;           // src0 rows
    const int64_t nr1 = ne11*ne12*ne13; // src1 rows

    //printf(&quot;nr0 = %lld, nr1 = %lld\n&quot;, nr0, nr1);

    // distribute the thread work across the inner or outer loop based on which one is larger

    const int64_t nth0 = nr0 &gt; nr1 ? nth : 1; // parallelize by src0 rows
    const int64_t nth1 = nr0 &gt; nr1 ? 1 : nth; // parallelize by src1 rows

    const int64_t ith0 = ith % nth0;
    const int64_t ith1 = ith / nth0;

    const int64_t dr0 = (nr0 + nth0 - 1)/nth0;
    const int64_t dr1 = (nr1 + nth1 - 1)/nth1;

    const int64_t ir010 = dr0*ith0;
    const int64_t ir011 = MIN(ir010 + dr0, nr0);

    const int64_t ir110 = dr1*ith1;
    const int64_t ir111 = MIN(ir110 + dr1, nr1);

    //printf(&quot;ir010 = %6lld, ir011 = %6lld, ir110 = %6lld, ir111 = %6lld\n&quot;, ir010, ir011, ir110, ir111);

    // threads with no work simply yield (not sure if it helps)
    if (ir010 &gt;= ir011 || ir110 &gt;= ir111) {
        sched_yield();
        return;
    }

    assert(ne12 % ne02 == 0);
    assert(ne13 % ne03 == 0);

    // block-tiling attempt
    const int64_t blck_0 = 16;
    const int64_t blck_1 = 16;

    // attempt to reduce false-sharing (does not seem to make a difference)
    float tmp[16];

    for (int64_t iir1 = ir110; iir1 &lt; ir111; iir1 += blck_1) {
        for (int64_t iir0 = ir010; iir0 &lt; ir011; iir0 += blck_0) {
            for (int64_t ir1 = iir1; ir1 &lt; iir1 + blck_1 &amp;&amp; ir1 &lt; ir111; ++ir1) {
                const int64_t i13 = (ir1/(ne12*ne11));
                const int64_t i12 = (ir1 - i13*ne12*ne11)/ne11;
                const int64_t i11 = (ir1 - i13*ne12*ne11 - i12*ne11);

                // broadcast src0 into src1
                const int64_t i03 = i13/r3;
                const int64_t i02 = i12/r2;

                const int64_t i1 = i11;
                const int64_t i2 = i12;
                const int64_t i3 = i13;

                const char * src0_row = (const char *) src0-&gt;data + (0 + i02*nb02 + i03*nb03);

                // desc: when src1 is not a contiguous memory block we have to calculate the offset using the strides
                //       if it is, then we have either copied the data to params-&gt;wdata and made it contiguous or we are using
                //       the original src1 data pointer, so we should index using the indices directly
                // TODO: this is a bit of a hack, we should probably have a better way to handle this
                const char * src1_col = (const char *) wdata +
                    (src1_cont || src1-&gt;type != vec_dot_type
                     ? (i11      + i12*ne11 + i13*ne12*ne11)*row_size
                     : (i11*nb11 + i12*nb12 + i13*nb13));

                float * dst_col = (float *) ((char *) dst-&gt;data + (i1*nb1 + i2*nb2 + i3*nb3));

                //for (int64_t ir0 = iir0; ir0 &lt; iir0 + blck_0 &amp;&amp; ir0 &lt; ir011; ++ir0) {
                //    vec_dot(ne00, &amp;dst_col[ir0], src0_row + ir0*nb01, src1_col);
                //}

                for (int64_t ir0 = iir0; ir0 &lt; iir0 + blck_0 &amp;&amp; ir0 &lt; ir011; ++ir0) {
                    vec_dot(ne00, &amp;tmp[ir0 - iir0], src0_row + ir0*nb01, src1_col);
                }
                memcpy(&amp;dst_col[iir0], tmp, (MIN(iir0 + blck_0, ir011) - iir0)*sizeof(float));
            }
        }
    }
}
</code></pre>
<p>è¿™ä¸€å¼€å¤´å°±æ¥ä¸ª<code>GGML_TENSOR_BINARY_OP_LOCALS</code>æ˜¯ä»€ä¹ˆä¸œè¥¿ï¼Ÿ<code>ne0</code>ï¼Œ<code>ne01</code>ç­‰ç­‰è¿™äº›å˜é‡åˆæ˜¯å“ªé‡Œæ¥çš„ï¼Ÿä¸€å¼€å§‹çœ‹åˆ°è¿™é‡Œä¸€å®šä¸€å¤´é›¾æ°´ã€‚<br>
æˆ‘ä»¬è¿½æ ¹æº¯æºï¼Œçœ‹çœ‹å®å®šä¹‰åˆ°åº•åšäº†ä»€ä¹ˆã€‚</p>
<pre><code class="language-c++">// in ggml.h
#define GGML_TENSOR_LOCALS_1(type, prefix, pointer, array) \
    const type prefix##0 = (pointer)-&gt;array[0]; \
    GGML_UNUSED(prefix##0);
#define GGML_TENSOR_LOCALS_2(type, prefix, pointer, array) \
    GGML_TENSOR_LOCALS_1    (type, prefix, pointer, array) \
    const type prefix##1 = (pointer)-&gt;array[1]; \
    GGML_UNUSED(prefix##1);
#define GGML_TENSOR_LOCALS_3(type, prefix, pointer, array) \
    GGML_TENSOR_LOCALS_2    (type, prefix, pointer, array) \
    const type prefix##2 = (pointer)-&gt;array[2]; \
    GGML_UNUSED(prefix##2);
#define GGML_TENSOR_LOCALS(type, prefix, pointer, array) \
    GGML_TENSOR_LOCALS_3  (type, prefix, pointer, array) \
    const type prefix##3 = (pointer)-&gt;array[3]; \
    GGML_UNUSED(prefix##3);
</code></pre>
<pre><code class="language-c++">// in ggml.c
#define GGML_TENSOR_BINARY_OP_LOCALS \
    GGML_TENSOR_LOCALS(int64_t, ne0, src0, ne) \
    GGML_TENSOR_LOCALS(size_t,  nb0, src0, nb) \
    GGML_TENSOR_LOCALS(int64_t, ne1, src1, ne) \
    GGML_TENSOR_LOCALS(size_t,  nb1, src1, nb) \
    GGML_TENSOR_LOCALS(int64_t, ne,  dst,  ne) \
    GGML_TENSOR_LOCALS(size_t,  nb,  dst,  nb)
</code></pre>
<p>åŸæ¥ï¼Œè¿™é‡Œæ˜¯è´Ÿè´£å®šä¹‰äº†ä¸€å †å±€éƒ¨å˜é‡ã€‚<br>
<code>GGML_TENSOR_LOCALS_1(type, prefix, pointer, array)</code>è´Ÿè´£å®šä¹‰<code>prefix##0</code>å±€éƒ¨å˜é‡æ¥å­˜å‚¨<code>(pointer)-&gt;array[0]</code>çš„å€¼ã€‚<br>
æ³¨æ„è¿™é‡Œçš„<code>##</code>çš„ä½œç”¨æ˜¯å°†ä¸¤ä¸ªæ ‡è®°æ‹¼æ¥æˆä¸€ä¸ªæ–°çš„æ ‡è®°ã€‚åœ¨å®å®šä¹‰ä¸­ï¼Œè¿™ä¸ªç‰¹æ€§å¯ä»¥ç”¨æ¥åŠ¨æ€ç”Ÿæˆæ ‡è¯†ç¬¦ï¼Œä»¥ç®€åŒ–ä»£ç å’Œæé«˜ä»£ç çš„çµæ´»æ€§ã€‚<code>prefix##0</code> ä½¿ç”¨äº†<code>##</code>ï¼Œå…¶ç›®çš„æ˜¯å°† <code>prefix</code> å’Œ <code>0</code> æ‹¼æ¥æˆä¸€ä¸ªæ–°çš„æ ‡è®°ã€‚å‡è®¾<code>prefix</code>æ˜¯<code>ne</code>ï¼Œé‚£ä¹ˆ<code>prefix##0</code>å°±ä¼šå˜æˆ<code>ne0</code>ï¼Œè¿™æ˜¯ä¸€ä¸ªåˆæ³•çš„æ ‡è¯†ç¬¦ã€‚å‡½æ•°é‡Œé¢æ‰€ä½¿ç”¨çš„é‚£äº›æ–°çš„å±€éƒ¨å˜é‡ï¼ˆçš„å˜é‡åï¼‰å°±æ˜¯è¿™ä¹ˆæ¥çš„ã€‚<br>
ä»<code>GGML_TENSOR_LOCALS</code>è°ƒç”¨åˆ°<code>GGML_TENSOR_LOCALS_3</code>å†åˆ°<code>GGML_TENSOR_LOCALS_2</code>å†åˆ°<code>GGML_TENSOR_LOCALS_1</code>ï¼Œ<code>prefix##0</code>åˆ°<code>prefix##3</code>ä¾æ¬¡è¢«<code>(pointer)-&gt;array[0]</code>åˆ°<code>(pointer)-&gt;array[3]</code>èµ‹å€¼åˆå§‹åŒ–ã€‚<br>
<code>GGML_TENSOR_BINARY_OP_LOCALS</code>åˆ™é’ˆå¯¹<code>src0</code>ï¼Œ<code>src1</code>å’Œ<code>dst</code>çš„<code>ne</code>å’Œ<code>nb</code>åˆ†åˆ«ä½¿ç”¨äº†<code>GGML_TENSOR_LOCALS</code>ï¼Œå°±æœ‰äº†æˆ‘ä»¬åœ¨å‡½æ•°ä¸­çœ‹åˆ°çš„é‚£ä¸€å †å±€éƒ¨å˜é‡äº†ã€‚<br>
ä¹‹åï¼Œæ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨ OpenCL åŠ é€Ÿï¼ˆCLBLASTï¼‰æˆ– BLAS åº“ï¼ˆOpenBLAS/Accelerateï¼‰ã€‚å¦‚æœå¯ä»¥ï¼Œåˆ™è°ƒç”¨ç›¸åº”çš„çŸ©é˜µä¹˜æ³•å‡½æ•°åŠ é€Ÿè®¡ç®—ã€‚</p>
<pre><code class="language-c++">#if defined(GGML_USE_CLBLAST)
    if (ggml_cl_can_mul_mat(src0, src1, dst)) {
        if (params-&gt;ith == 0 &amp;&amp; params-&gt;type == GGML_TASK_COMPUTE) {
            ggml_cl_mul_mat(src0, src1, dst, params-&gt;wdata, params-&gt;wsize);
        }
        return;
    }
#endif

#if defined(GGML_USE_ACCELERATE) || defined(GGML_USE_OPENBLAS)
    if (ggml_compute_forward_mul_mat_use_blas(src0, src1, dst)) {
        if (params-&gt;ith != 0) {
            return;
        }

        if (params-&gt;type == GGML_TASK_INIT) {
            return;
        }

        if (params-&gt;type == GGML_TASK_FINALIZE) {
            return;
        }

        for (int64_t i13 = 0; i13 &lt; ne13; i13++) {
            for (int64_t i12 = 0; i12 &lt; ne12; i12++) {
                // broadcast src0 into src1 across 2nd,3rd dimension
                const int64_t i03 = i13/r3;
                const int64_t i02 = i12/r2;

                const void  * x = (char *)            src0-&gt;data + i02*nb02 + i03*nb03;
                const float * y = (float *) ((char *) src1-&gt;data + i12*nb12 + i13*nb13);

                float * d = (float *) ((char *) dst-&gt;data + i12*nb2 + i13*nb3);

                if (type != GGML_TYPE_F32) {
                            float * const wdata    = params-&gt;wdata;
                    ggml_to_float_t const to_float = type_traits[type].to_float;

                    size_t id = 0;
                    for (int64_t i01 = 0; i01 &lt; ne01; ++i01) {
                        to_float((const char *) x + i01*nb01, wdata + id, ne00);
                        id += ne00;
                    }

                    assert(id*sizeof(float) &lt;= params-&gt;wsize);
                    x = wdata;
                }

                cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasTrans,
                        ne11, ne01, ne10,
                        1.0f,    y, ne10,
                                 x, ne00,
                        0.0f,    d, ne01);
            }
        }

        //printf(&quot;CBLAS = %f ms, %d x %d x %d x %d\n&quot;, (ggml_perf_time_us() - t0)/1000.0, ne0, ne1, ne2, ne3);

        return;
    }
#endif
</code></pre>
<ul>
<li>GGML_TASK_INITï¼šåˆå§‹åŒ–é˜¶æ®µï¼Œç”¨æ¥å°† src1 ä¸­çš„ float ç±»å‹æ•°æ®è½¬æ¢ä¸ºé€‚åˆè¿›è¡ŒçŸ©é˜µä¹˜æ³•çš„ç±»å‹ï¼ˆvec_dot_typeï¼‰ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨ä¸€ä¸ªä¸´æ—¶ç¼“å†²åŒº wdata ä¸­ï¼Œä»¥ä¾¿ä¹‹åçš„è®¡ç®—ä½¿ç”¨ã€‚</li>
<li>GGML_TASK_FINALIZEï¼šç»“æŸé˜¶æ®µï¼Œä»€ä¹ˆä¹Ÿä¸åšï¼Œåªæ˜¯æ ‡è®°è®¡ç®—ç»“æŸã€‚</li>
</ul>
<pre><code class="language-c++">    if (params-&gt;type == GGML_TASK_INIT) {
        if (src1-&gt;type != vec_dot_type) {
            char * wdata = params-&gt;wdata;
            const size_t row_size = ne10*ggml_type_size(vec_dot_type)/ggml_blck_size(vec_dot_type);

            for (int64_t i13 = 0; i13 &lt; ne13; ++i13) {
                for (int64_t i12 = 0; i12 &lt; ne12; ++i12) {
                    for (int64_t i11 = 0; i11 &lt; ne11; ++i11) {
                        from_float_to_vec_dot((float *)((char *) src1-&gt;data + i13*nb13 + i12*nb12 + i11*nb11), (void *) wdata, ne10);
                        wdata += row_size;
                    }
                }
            }
        }

        return;
    }
</code></pre>
<p>åé¢è¿™éƒ¨åˆ†ä»£ç ä¸»è¦æ˜¯ä»»åŠ¡çš„åˆ†é…ã€‚<br>
ï¼ˆ<code>nth0</code>å’Œ<code>nth1</code>ï¼‰å…ˆæ˜¯çœ‹ä¸€ä¸‹<code>src0</code>å’Œ<code>src1</code>è°çš„è¡Œæ•°å¤šï¼ˆ<code>nr0</code>å’Œ<code>nr1</code>ï¼‰ï¼Œè°å¤šå°±æŒ‰è°æ¥å¹¶è¡Œã€‚<br>
ï¼ˆ<code>ith0</code>å’Œ<code>ith1</code>ï¼‰ç„¶åï¼Œè®¡ç®—å½“å‰çº¿ç¨‹åº”è¯¥å¤„ç†<code>src0</code>å’Œ<code>src</code>çš„å“ªéƒ¨åˆ†ã€‚<br>
æˆ‘ä»¬å¯ä»¥è¿™æ ·ç†è§£è¿™ä¸ªå…¬å¼ï¼šå‡è®¾<code>nth</code>æ˜¯4ï¼Œ</p>
<ul>
<li>å¦‚æœ<code>nth0</code>æ˜¯4ï¼Œé‚£ä¹ˆ<code>ith0</code>å°±æ˜¯<code>[0, 3]</code>ä¹Ÿå°±æ˜¯å¤„ç†å››ä¸ªä¸åŒçš„éƒ¨åˆ†ï¼Œ<code>ith1</code>åˆ™åªèƒ½æ˜¯0</li>
<li>å¦‚æœ<code>nth0</code>æ˜¯1ï¼Œé‚£ä¹ˆ<code>ith0</code>åªèƒ½æ˜¯0ï¼Œè€Œ<code>ith1</code>å¯ä»¥æ˜¯<code>[0, 3]</code>ã€‚</li>
</ul>
<p>ï¼ˆ<code>dr0</code>å’Œ<code>dr1</code>ï¼‰ç„¶åï¼Œè®¡ç®—æ¯ä¸ªçº¿ç¨‹åº”è¯¥åˆ†åˆ«å¤„ç†å¤šå°‘è¡Œ<code>src0</code>å’Œ<code>src1</code>ã€‚è¿™é‡Œå°±æ˜¯ç”¨æ€»è¡Œæ•°å¯¹<code>nth0</code>å’Œ<code>nth1</code>åšå•†çš„å‘ä¸Šå–æ•´ã€‚<br>
ï¼ˆ<code>ir010</code>å’Œ<code>ir011</code>ï¼‰æœ‰äº†è´Ÿè´£çš„éƒ¨åˆ†ï¼ˆ<code>ith0</code>ï¼‰ä»¥åŠæ¯ä¸ªéƒ¨åˆ†çš„è¡Œæ•°<code>dr0</code>ï¼Œæˆ‘ä»¬å°±å¯ä»¥è®¡ç®—çº¿ç¨‹å¯¹<code>src0</code>çš„å¤„ç†èŒƒå›´äº†ï¼Œå³å¤„ç†<code>src0</code>çš„èµ·å§‹è¡Œï¼ˆ<code>dr0*ith0</code>ï¼‰å’Œç»ˆæ­¢è¡Œï¼ˆ<code>MIN(ir010 + dr0, nr0)</code>ï¼‰ã€‚<br>
ï¼ˆ<code>ir110</code>å’Œ<code>ir111</code>ï¼‰åŒä¸Šï¼Œçº¿ç¨‹å¤„ç† <code>src1</code> çš„èµ·å§‹è¡Œå’Œç»“æŸè¡Œã€‚</p>
<pre><code class="language-c++">    const void * wdata    = (src1-&gt;type == vec_dot_type) ? src1-&gt;data : params-&gt;wdata;
    const size_t row_size = ne10*ggml_type_size(vec_dot_type)/ggml_blck_size(vec_dot_type);

    const int64_t nr0 = ne01;           // src0 rows
    const int64_t nr1 = ne11*ne12*ne13; // src1 rows

    //printf(&quot;nr0 = %lld, nr1 = %lld\n&quot;, nr0, nr1);

    // distribute the thread work across the inner or outer loop based on which one is larger

    const int64_t nth0 = nr0 &gt; nr1 ? nth : 1; // parallelize by src0 rows
    const int64_t nth1 = nr0 &gt; nr1 ? 1 : nth; // parallelize by src1 rows

    const int64_t ith0 = ith % nth0;
    const int64_t ith1 = ith / nth0;

    const int64_t dr0 = (nr0 + nth0 - 1)/nth0;
    const int64_t dr1 = (nr1 + nth1 - 1)/nth1;

    const int64_t ir010 = dr0*ith0;
    const int64_t ir011 = MIN(ir010 + dr0, nr0);

    const int64_t ir110 = dr1*ith1;
    const int64_t ir111 = MIN(ir110 + dr1, nr1);
</code></pre>
<p>æ¥ä¸‹æ¥æ˜¯è®¡ç®—ã€‚é¦–å…ˆæ˜¯ä¸€ä¸ªçŸ©é˜µçš„åˆ†å—ã€‚å¤–ä¸¤å±‚å¾ªç¯æ¯æ¬¡åŠ <code>blck_0</code>å’Œ<code>blck_1</code>ï¼Œç„¶åå†…ä¸¤å±‚å¾ªç¯å¤„ç†è¯¥blockå†…çš„æ•°æ®ã€‚å…¶ä¸­ç¬¬ä¸‰å±‚å¾ªç¯å¢åŠ <code>src1</code>çš„ä¸€åˆ—ï¼Œç¬¬å››å±‚å¾ªç¯å¢åŠ <code>src0</code>çš„ä¸€è¡Œã€‚ç¬¬å››å±‚å¾ªç¯ç®—å®Œä¼šå¾—åˆ°ç»“æœçš„ä¸€åˆ—ï¼ˆé€šè¿‡<code>src1</code>çš„ä¸€åˆ—å’Œblockå†…<code>src0</code>çš„æ‰€æœ‰è¡Œï¼‰ï¼Œå†æŠŠç»“æœ<code>memcpy</code>åˆ°<code>dst_col</code>ã€‚</p>
<pre><code class="language-c++">    // block-tiling attempt
    const int64_t blck_0 = 16;
    const int64_t blck_1 = 16;

    // attempt to reduce false-sharing (does not seem to make a difference)
    float tmp[16];

    for (int64_t iir1 = ir110; iir1 &lt; ir111; iir1 += blck_1) {
        for (int64_t iir0 = ir010; iir0 &lt; ir011; iir0 += blck_0) {
            for (int64_t ir1 = iir1; ir1 &lt; iir1 + blck_1 &amp;&amp; ir1 &lt; ir111; ++ir1) {
                // -- snip --
                for (int64_t ir0 = iir0; ir0 &lt; iir0 + blck_0 &amp;&amp; ir0 &lt; ir011; ++ir0) {
                    vec_dot(ne00, &amp;tmp[ir0 - iir0], src0_row + ir0*nb01, src1_col);
                }
                memcpy(&amp;dst_col[iir0], tmp, (MIN(iir0 + blck_0, ir011) - iir0)*sizeof(float));
            }
        }
    }
</code></pre>
<p>ç¬¬ä¸‰å±‚å¾ªç¯å†…çš„å¼€å¤´æ˜¯ä¸€ä¸ªä¸€ç»´ç´¢å¼•åˆ°ä¸‰ç»´ç´¢å¼•çš„é‡æ–°è®¡ç®—ã€‚æˆ‘ä»¬å¯ä»¥è¿™æ ·ç†è§£è¿™ä¸ªå…¬å¼ï¼š</p>
<ul>
<li>ir1åœ¨ä¸‰ç»´å¼ é‡ä¸­é¦–å…ˆè·¨è¿‡äº†å¤šå°‘â€œæ·±åº¦â€ï¼ˆ<code>ne12 * ne11</code>è¡¨ç¤ºä¸€ä¸ªå®Œæ•´çš„å¹³é¢å¤§å°ï¼‰ã€‚æ‰€ä»¥è¿™æ­¥æ˜¯åœ¨ç¬¬ 2 ç»´åº¦ä¸Šè·¨è¿‡å¤šå°‘ä¸ªå®Œæ•´çš„<code>ne11 x ne12</code>å¤§å°çš„å¹³é¢ã€‚</li>
<li>åœ¨ç¡®å®š<code>i13</code>ä¹‹åï¼Œå‡å»i13å¯¹åº”çš„å±•å¹³ç´¢å¼•æ‰€è·¨è¿‡çš„éƒ¨åˆ†ï¼Œå‰©ä¸‹çš„å°±æ˜¯åœ¨<code>ne11 * ne12</code>è¿™ä¸ªå¹³é¢ä¸­çš„ä½ç½®ã€‚æ¥ä¸‹æ¥ï¼Œç”¨è¿™ä¸ªå‰©ä½™éƒ¨åˆ†é™¤ä»¥<code>ne11</code>ï¼Œå¾—åˆ°<code>i12</code>ï¼Œå³åœ¨ç¬¬ 1 ç»´åº¦ä¸Šçš„ç´¢å¼•ã€‚</li>
<li>æœ€åï¼Œå‡å»<code>i13</code>å’Œ<code>i12</code>è·¨è¿‡çš„éƒ¨åˆ†ï¼Œå‰©ä¸‹çš„å°±æ˜¯åœ¨ç¬¬ 0 ç»´åº¦ä¸Šçš„ç´¢å¼•<code>i11</code>ã€‚å®ƒè¡¨ç¤ºå½“å‰<code>ir1</code>åœ¨è¿™ä¸ªå¹³é¢ä¸­ï¼Œå…·ä½“çš„ç¬¬ 0 ç»´åº¦ä¸Šçš„ä½ç½®ã€‚</li>
</ul>
<pre><code class="language-c++">                const int64_t i13 = (ir1/(ne12*ne11));
                const int64_t i12 = (ir1 - i13*ne12*ne11)/ne11;
                const int64_t i11 = (ir1 - i13*ne12*ne11 - i12*ne11);
</code></pre>
<p>é€šè¿‡ r2 å’Œ r3 å°† src0 çš„è¾ƒå°ç»´åº¦å¹¿æ’­åˆ° src1ï¼Œä»¥é€‚åº”åè€…è¾ƒå¤§çš„ç»´åº¦ã€‚å…·ä½“åœ°è¯´ï¼Œi03 å’Œ i02 é€šè¿‡é™¤ä»¥å¹¿æ’­å› å­å°† src1 çš„ç´¢å¼•è½¬æ¢ä¸º src0 çš„ç´¢å¼•ï¼Œä»¥æ­£ç¡®åœ°åœ¨è¾ƒå¤§çš„å¼ é‡ä¸Šåº”ç”¨è¾ƒå°å¼ é‡çš„å€¼ã€‚<br>
ä¾‹å¦‚ï¼Œå¦‚æœ i13 = 4ï¼Œr3 = 3ï¼Œåˆ™ i03 = 4 / 3 = 1ï¼Œè¡¨ç¤º src0 çš„ç¬¬ 3 ç»´çš„ç¬¬ 1 ä¸ªå…ƒç´ ä¼šè¢«åº”ç”¨äº src1 çš„ç¬¬ 4 ä¸ªä½ç½®ã€‚</p>
<pre><code class="language-c++">                // broadcast src0 into src1
                const int64_t i03 = i13/r3;
                const int64_t i02 = i12/r2;
</code></pre>
<p>é’ˆå¯¹ src1 çš„å¤šç»´ç´¢å¼•ï¼ˆä¾‹å¦‚ i11, i12, i13ï¼‰ç›´æ¥æ˜ å°„åˆ° dst ä¸­çš„å¯¹åº”ç»´åº¦ä½ç½®ã€‚è¿™è¡¨æ˜åœ¨ dst ä¸­çš„å…ƒç´ å°†ä¸ src1 çš„è¿™äº›ç»´åº¦ä¸€ä¸€å¯¹åº”ã€‚</p>
<pre><code class="language-c++">                const int64_t i1 = i11;
                const int64_t i2 = i12;
                const int64_t i3 = i13;
</code></pre>
<p>è®¡ç®—<code>src0</code>çš„è¡Œèµ·å§‹ä½ç½®ï¼ˆblockä¸­çš„ç¬¬ä¸€è¡Œï¼‰ã€‚</p>
<pre><code class="language-c++">                const char * src0_row = (const char *) src0-&gt;data + (0 + i02*nb02 + i03*nb03);
</code></pre>
<p>è®¡ç®—<code>src1_col</code>å’Œ<code>dst_col</code>çš„ä½ç½®ã€‚è¿™é‡Œå¦‚æœ<code>src1</code>æ˜¯è¿ç»­çš„åˆ™ä½¿ç”¨<code>ne</code>å’Œ<code>row_size</code>æ¥è®¡ç®—ï¼Œå¦åˆ™ä½¿ç”¨<code>nb</code>æ¥è®¡ç®—ï¼ˆ<strong>ä¸ºä»€ä¹ˆï¼Ÿ</strong>ï¼‰ã€‚</p>
<pre><code class="language-c++">                const char * src1_col = (const char *) wdata +
                    (src1_cont || src1-&gt;type != vec_dot_type
                     ? (i11      + i12*ne11 + i13*ne12*ne11)*row_size
                     : (i11*nb11 + i12*nb12 + i13*nb13));

                float * dst_col = (float *) ((char *) dst-&gt;data + (i1*nb1 + i2*nb2 + i3*nb3));
</code></pre>
<p>æœ€åï¼Œç®—å‡ºç»“æœä¸­çš„ä¸€åˆ—ã€‚è¿™é‡Œä¾æ¬¡æŠŠblocké‡Œé¢<code>src0</code>çš„å„è¡Œä¸<code>src1</code>çš„å¯¹åº”åˆ—åšå‘é‡ç‚¹ä¹˜ï¼ŒæŠŠç»“æœæ”¾åˆ°<code>tmp</code>ä¸­ï¼Œå†æŠŠ<code>tmp</code>çš„ç»“æœæ”¾åˆ°<code>dst_col</code>å³å¯¹åº”åˆ—ä¸­ã€‚</p>
<pre><code class="language-c++">                for (int64_t ir0 = iir0; ir0 &lt; iir0 + blck_0 &amp;&amp; ir0 &lt; ir011; ++ir0) {
                    vec_dot(ne00, &amp;tmp[ir0 - iir0], src0_row + ir0*nb01, src1_col);
                }
                memcpy(&amp;dst_col[iir0], tmp, (MIN(iir0 + blck_0, ir011) - iir0)*sizeof(float));
</code></pre>
<p>è¿™æ ·ä¸€æ¥ï¼Œ<code>ggml.c</code>ä¸­çš„<code>ggml_compute_forward_mul_mat</code>å°±ç»“æŸäº†ã€‚</p>
<h2 id="ggml_cuda_mul_mat">ggml_cuda_mul_mat</h2>
<p>å¯¹äº<code>GGML_USE_CUBLAS</code>ä¸º<code>True</code>çš„æƒ…å†µï¼Œ<code>GGML_OP_MUL_MAT</code>ä¼šè°ƒç”¨åˆ°<code>ggml_cuda_mul_mat</code>ã€‚<br>
<code>ggml_cuda_mul_mat</code>ä¼šé€šè¿‡æ£€æŸ¥è¾“å…¥å¼ é‡çš„ç±»å‹ã€å†…å­˜å¸ƒå±€ã€è®¡ç®—èƒ½åŠ›ç­‰æ¡ä»¶ï¼Œæ¥é€‰æ‹©æœ€é€‚åˆçš„çŸ©é˜µä¹˜æ³•è®¡ç®—æ–¹æ³•ã€‚</p>
<pre><code class="language-c++">static void ggml_cuda_mul_mat(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {
    const bool all_on_device =
        (src0-&gt;backend == GGML_BACKEND_GPU || src0-&gt;backend == GGML_BACKEND_GPU_SPLIT) &amp;&amp;
        (src1-&gt;backend == GGML_BACKEND_GPU) &amp;&amp;
        ( dst-&gt;backend == GGML_BACKEND_GPU);

    const bool split = src0-&gt;backend == GGML_BACKEND_GPU_SPLIT;

    int64_t min_compute_capability = INT_MAX;
    for (int64_t id = 0; id &lt; g_device_count; ++id) {
        if (min_compute_capability &gt; g_compute_capabilities[id] &amp;&amp; g_tensor_split[id] &lt; (id + 1 &lt; g_device_count ? g_tensor_split[id + 1] : 1.0f)) {
            min_compute_capability = g_compute_capabilities[id];
        }
    }

#ifdef CUDA_USE_TENSOR_CORES
    const bool use_tensor_cores = true;
#else
    const bool use_tensor_cores = false;
#endif

    // debug helpers
    //printf(&quot;src0: %8d %8d %8d %8d\n&quot;, src0-&gt;ne[0], src0-&gt;ne[1], src0-&gt;ne[2], src0-&gt;ne[3]);
    //printf(&quot;      %8d %8d %8d %8d\n&quot;, src0-&gt;nb[0], src0-&gt;nb[1], src0-&gt;nb[2], src0-&gt;nb[3]);
    //printf(&quot;src1: %8d %8d %8d %8d\n&quot;, src1-&gt;ne[0], src1-&gt;ne[1], src1-&gt;ne[2], src1-&gt;ne[3]);
    //printf(&quot;      %8d %8d %8d %8d\n&quot;, src1-&gt;nb[0], src1-&gt;nb[1], src1-&gt;nb[2], src1-&gt;nb[3]);
    //printf(&quot;src0 is contiguous %d, transposed %d, type = %s, name = %s\n&quot;, ggml_is_contiguous(src0), ggml_is_transposed(src0), ggml_type_name(src0-&gt;type), src0-&gt;name);
    //printf(&quot;src1 is contiguous %d, transposed %d, type = %s, name = %s\n&quot;, ggml_is_contiguous(src1), ggml_is_transposed(src1), ggml_type_name(src1-&gt;type), src1-&gt;name);

    if (!split &amp;&amp; all_on_device &amp;&amp; !use_tensor_cores &amp;&amp; src0-&gt;type == GGML_TYPE_F16 &amp;&amp; ggml_is_permuted(src0) &amp;&amp; ggml_is_permuted(src1) &amp;&amp; src1-&gt;ne[1] == 1) {
        // KQ single-batch
        ggml_cuda_mul_mat_vec_p021(src0, src1, dst);
    } else if (!split &amp;&amp; all_on_device &amp;&amp; !use_tensor_cores &amp;&amp; src0-&gt;type == GGML_TYPE_F16 &amp;&amp; !ggml_is_contiguous(src0) &amp;&amp; !ggml_is_transposed(src1) &amp;&amp; src1-&gt;ne[1] == 1) {
        // KQV single-batch
        ggml_cuda_mul_mat_vec_nc(src0, src1, dst);
    } else if (!split &amp;&amp; all_on_device &amp;&amp; use_tensor_cores &amp;&amp; src0-&gt;type == GGML_TYPE_F16 &amp;&amp; src1-&gt;type == GGML_TYPE_F32 &amp;&amp; !ggml_is_transposed(src0) &amp;&amp; !ggml_is_transposed(src1)) {
        // KQ + KQV multi-batch
        ggml_cuda_mul_mat_mat_batched_cublas(src0, src1, dst);
    } else if (src0-&gt;type == GGML_TYPE_F32) {
        ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_mul_mat_cublas, false);
    } else if (ggml_is_quantized(src0-&gt;type) || src0-&gt;type == GGML_TYPE_F16) {
        if (src1-&gt;ne[1] == 1 &amp;&amp; src0-&gt;ne[0] % GGML_CUDA_DMMV_X == 0) {
#ifdef GGML_CUDA_FORCE_DMMV
            const bool use_mul_mat_vec_q = false;
#else
            const bool use_mul_mat_vec_q = min_compute_capability &gt;= MIN_CC_DP4A &amp;&amp; ggml_is_quantized(src0-&gt;type);
#endif // GGML_CUDA_FORCE_DMMV

            if (use_mul_mat_vec_q) {
                ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_mul_mat_vec_q, true);
            } else {
                ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_dequantize_mul_mat_vec, false);
            }
        } else {
            bool use_mul_mat_q = min_compute_capability &gt;= MIN_CC_DP4A &amp;&amp; ggml_is_quantized(src0-&gt;type);

            // when tensor cores are available, use them for large batch size
            // ref: https://github.com/ggerganov/llama.cpp/pull/3776
            if (use_tensor_cores &amp;&amp; min_compute_capability &gt;= CC_VOLTA &amp;&amp; src1-&gt;ne[1] &gt; MMQ_MAX_BATCH_SIZE) {
                use_mul_mat_q = false;
            }

            if (use_mul_mat_q) {
                ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_mul_mat_q, true);
            } else {
                ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_mul_mat_cublas, false);
            }
        }
    } else {
        GGML_ASSERT(false);
    }
}
</code></pre>
<p>å¯¹äº<code>src0</code>æ˜¯é‡åŒ–ç±»å‹ï¼Œä¸”<code>src1</code>æ˜¯åˆ—å‘é‡çš„æƒ…å†µï¼Œä»£ç ä¼šæ ¹æ®<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities">GPUè®¡ç®—èƒ½åŠ›</a>é€‰æ‹©æ˜¯ç›´æ¥ä½¿ç”¨é‡åŒ–çš„çŸ©é˜µä¹˜æ³•ï¼ˆ<code>ggml_cuda_op_mul_mat_vec_q</code>ï¼‰ï¼Œè¿˜æ˜¯å…ˆè¿›è¡Œåé‡åŒ–ï¼ˆ<code>ggml_cuda_op_dequantize_mul_mat_vec</code>ï¼‰ã€‚</p>
<pre><code class="language-c++">            if (use_mul_mat_vec_q) {
                ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_mul_mat_vec_q, true);
            } else {
                ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_dequantize_mul_mat_vec, false);
            }
</code></pre>
<p><code>ggml_cuda_op_mul_mat</code>çš„å‡½æ•°ç­¾åå¦‚ä¸‹ã€‚æ³¨æ„è¿™é‡Œæœ€åä¸€ä¸ªå‚æ•°<code>covert_src1_to_q8_1</code>å¦‚æœä¸º<code>true</code>ï¼Œåˆ™ä¼šå°è¯•è°ƒç”¨<code>quantize_q8_1</code>å¯¹è¾“å…¥<code>src1</code>è¿›è¡Œé‡åŒ–ã€‚</p>
<pre><code class="language-c++">static void ggml_cuda_op_mul_mat(
    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, ggml_cuda_op_mul_mat_t op,
    const bool convert_src1_to_q8_1) 
</code></pre>
<h3 id="ggml_cuda_op_dequantize_mul_mat_vec">ggml_cuda_op_dequantize_mul_mat_vec</h3>
<p><code>ggml_cuda_op_dequantize_mul_mat_vec</code>åŸºäºé‡åŒ–ç±»å‹é€‰æ‹©åˆé€‚çš„å‡½æ•°ã€‚</p>
<pre><code class="language-c++">inline void ggml_cuda_op_dequantize_mul_mat_vec(
    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, const char * src0_dd_i, const float * src1_ddf_i,
    const char * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const int64_t src1_ncols,
    const int64_t src1_padded_row_size, const cudaStream_t &amp; stream) {

    const int64_t ne00 = src0-&gt;ne[0];
    const int64_t row_diff = row_high - row_low;

    // on some GPUs it is faster to convert src1 to half and to use half precision intrinsics
#ifdef GGML_CUDA_F16
    size_t ash;
    dfloat * src1_dfloat = nullptr; // dfloat == half

    bool src1_convert_f16 = src0-&gt;type == GGML_TYPE_Q4_0 || src0-&gt;type == GGML_TYPE_Q4_1 ||
        src0-&gt;type == GGML_TYPE_Q5_0 || src0-&gt;type == GGML_TYPE_Q5_1 ||
        src0-&gt;type == GGML_TYPE_Q8_0 || src0-&gt;type == GGML_TYPE_F16;

    if (src1_convert_f16) {
        src1_dfloat = (half *) ggml_cuda_pool_malloc(ne00*sizeof(half), &amp;ash);
        ggml_cpy_f32_f16_cuda((const char *) src1_ddf_i, (char *) src1_dfloat, ne00,
                                ne00, 1, sizeof(float), 0, 0,
                                ne00, 1, sizeof(half),  0, 0, stream);
    }
#else
    const dfloat * src1_dfloat = (const dfloat *) src1_ddf_i; // dfloat == float, no conversion
#endif // GGML_CUDA_F16

    switch (src0-&gt;type) {
        case GGML_TYPE_Q4_0:
            dequantize_mul_mat_vec_q4_0_cuda(src0_dd_i, src1_dfloat, dst_dd_i, ne00, row_diff, stream);
            break;
        case GGML_TYPE_Q4_1:
            dequantize_mul_mat_vec_q4_1_cuda(src0_dd_i, src1_dfloat, dst_dd_i, ne00, row_diff, stream);
            break;
        case GGML_TYPE_Q5_0:
            dequantize_mul_mat_vec_q5_0_cuda(src0_dd_i, src1_dfloat, dst_dd_i, ne00, row_diff, stream);
            break;
        case GGML_TYPE_Q5_1:
            dequantize_mul_mat_vec_q5_1_cuda(src0_dd_i, src1_dfloat, dst_dd_i, ne00, row_diff, stream);
            break;
        case GGML_TYPE_Q8_0:
            dequantize_mul_mat_vec_q8_0_cuda(src0_dd_i, src1_dfloat, dst_dd_i, ne00, row_diff, stream);
            break;
        case GGML_TYPE_Q2_K:
            dequantize_mul_mat_vec_q2_K_cuda(src0_dd_i, src1_ddf_i, dst_dd_i, ne00, row_diff, stream);
            break;
        case GGML_TYPE_Q3_K:
            dequantize_mul_mat_vec_q3_K_cuda(src0_dd_i, src1_ddf_i, dst_dd_i, ne00, row_diff, stream);
            break;
        case GGML_TYPE_Q4_K:
            dequantize_mul_mat_vec_q4_K_cuda(src0_dd_i, src1_ddf_i, dst_dd_i, ne00, row_diff, stream);
            break;
        case GGML_TYPE_Q5_K:
            dequantize_mul_mat_vec_q5_K_cuda(src0_dd_i, src1_ddf_i, dst_dd_i, ne00, row_diff, stream);
            break;
        case GGML_TYPE_Q6_K:
            dequantize_mul_mat_vec_q6_K_cuda(src0_dd_i, src1_ddf_i, dst_dd_i, ne00, row_diff, stream);
            break;
        case GGML_TYPE_F16:
            convert_mul_mat_vec_f16_cuda(src0_dd_i, src1_dfloat, dst_dd_i, ne00, row_diff, stream);
            break;
        default:
            GGML_ASSERT(false);
            break;
    }

#ifdef GGML_CUDA_F16
    if (src1_convert_f16) {
        ggml_cuda_pool_free(src1_dfloat, ash);
    }
#endif // GGML_CUDA_F16

    (void) src1;
    (void) dst;
    (void) src1_ddq_i;
    (void) src1_ncols;
    (void) src1_padded_row_size;
}
</code></pre>
<p>äº‹å®ä¸Šï¼Œè¿™äº›å‡½æ•°éƒ½ä¼šè°ƒç”¨åˆ°<code>dequantize_mul_mat_vec</code>ï¼Œåªæ˜¯é€‰æ‹©äº†ä¸åŒçš„å‡½æ•°æ¨¡ç‰ˆã€‚<br>
ä»¥<code>GGML_TYPE_Q4_0</code>ä¸ºä¾‹ï¼Œè°ƒç”¨çš„æ˜¯<code>dequantize_mul_mat_vec&lt;QK4_0, QR4_0, dequantize_q4_0&gt;</code>ã€‚</p>
<pre><code class="language-c++">static void dequantize_mul_mat_vec_q4_0_cuda(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, cudaStream_t stream) {
    GGML_ASSERT(ncols % GGML_CUDA_DMMV_X == 0);
    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;
    // the number of rows may exceed maximum grid size in the y or z dimensions, use the x dimension instead
    const dim3 block_nums(block_num_y, 1, 1);
    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);
    dequantize_mul_mat_vec&lt;QK4_0, QR4_0, dequantize_q4_0&gt;
        &lt;&lt;&lt;block_nums, block_dims, 0, stream&gt;&gt;&gt;(vx, y, dst, ncols, nrows);
}
</code></pre>
<p>å…¶ä¸­çš„<code>dequantize_q4_0</code>æ˜¯<code>GGML_TYPE_Q4_0</code>ç±»å‹çš„åé‡åŒ–å‡½æ•°ã€‚</p>
<pre><code class="language-c++">static __device__ __forceinline__ void dequantize_q4_0(const void * vx, const int ib, const int iqs, dfloat2 &amp; v){
    const block_q4_0 * x = (const block_q4_0 *) vx;

    const dfloat d = x[ib].d;

    const int vui = x[ib].qs[iqs];

    v.x = vui &amp; 0xF;
    v.y = vui &gt;&gt; 4;

#ifdef GGML_CUDA_F16
    v = __hsub2(v, {8.0f, 8.0f});
    v = __hmul2(v, {d, d});
#else
    v.x = (v.x - 8.0f) * d;
    v.y = (v.y - 8.0f) * d;
#endif // GGML_CUDA_F16
}
</code></pre>
<p>åœ¨æœ€ç»ˆè°ƒç”¨åˆ°çš„<code>dequantize_mul_mat_vec</code>ä¸­ï¼Œ<code>blockIdx.x*blockDim.y + threadIdx.y</code>ç”¨äºæŒ‡å®šè´Ÿè´£çš„<code>row</code>ï¼Œ<code>threadIdx.x</code>ä½œä¸º<code>tid</code>ç”¨äºè®¡ç®—æ‰€è´Ÿè´£çš„<code>col</code>ã€‚ä»¥<code>iter_stride</code>ä¸ºæ­¥é•¿ï¼Œæ¯æ¬¡è¿­ä»£å¤„ç†<code>vals_per_iter</code>ä¸ªæ•°æ®ï¼ŒåŒæ—¶å•ä¸ªè¿­ä»£ä¸­åˆæ¯æ¬¡å¤„ç†ä¸¤ä¸ªæ•°æ®ï¼ˆ<code>j += 2</code>ï¼‰ã€‚<br>
å¤„ç†æ—¶ï¼Œå…ˆå¯¹<code>vx</code>åšåé‡åŒ–ï¼Œç»“æœæ”¾ç½®åˆ°<code>v</code>ï¼Œç„¶åå†ç”¨<code>v</code>å’Œ<code>y</code>åšç›¸ä¹˜ï¼Œç»“æœç´¯åŠ åˆ°<code>tmp</code>ä¸­ã€‚<br>
æ¥ç€ï¼Œæ ¹æ®åˆ’åˆ†<code>const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);</code>ï¼Œæˆ‘ä»¬çŸ¥é“<code>threadIdx.x</code>çš„èŒƒå›´å…¶å®å°±æ˜¯0~31ï¼Œå› æ­¤å¯ä»¥åšä¸€ä¸ªwarp shuffleï¼ˆ<code>tmp += __shfl_xor_sync(0xffffffff, tmp, mask, 32);</code>ï¼‰æŠŠç»“æœç´¯åŠ ï¼Œæœ€å<code>tid == 0</code>çš„çº¿ç¨‹è´Ÿè´£æŠŠç»“æœæ”¾ç½®åˆ°<code>dst[row]</code>ä¸­ã€‚</p>
<pre><code class="language-c++">template &lt;int qk, int qr, dequantize_kernel_t dequantize_kernel&gt;
static __global__ void dequantize_mul_mat_vec(const void * __restrict__ vx, const dfloat * __restrict__ y, float * __restrict__ dst, const int ncols, const int nrows) {
    // qk = quantized weights per x block
    // qr = number of quantized weights per data value in x block
    const int row = blockIdx.x*blockDim.y + threadIdx.y;

    if (row &gt;= nrows) {
        return;
    }

    const int tid = threadIdx.x;

    const int iter_stride = 2*GGML_CUDA_DMMV_X;
    const int vals_per_iter = iter_stride / WARP_SIZE; // num quantized vals per thread and i iter
    const int y_offset = qr == 1 ? 1 : qk/2;

// partial sum for each thread
#ifdef GGML_CUDA_F16
    half2 tmp = {0.0f, 0.0f}; // two sums for f16 to take advantage of half2 intrinsics
#else
    float tmp = 0.0f;
#endif // GGML_CUDA_F16

    for (int i = 0; i &lt; ncols; i += iter_stride) {
        const int col = i + vals_per_iter*tid;
        const int ib = (row*ncols + col)/qk; // x block index
        const int iqs = (col%qk)/qr; // x quant index
        const int iybs = col - col%qk; // y block start index

// processing &gt;2 values per i iter is faster for fast GPUs
#pragma unroll
        for (int j = 0; j &lt; vals_per_iter; j += 2) {
            // process 2 vals per j iter

            // dequantize
            // for qr = 2 the iqs needs to increase by 1 per j iter because 2 weights per data val
            dfloat2 v;
            dequantize_kernel(vx, ib, iqs + j/qr, v);

            // matrix multiplication
            // for qr = 2 the y index needs to increase by 1 per j iter because of y_offset = qk/2
#ifdef GGML_CUDA_F16
            tmp += __hmul2(v, {
                y[iybs + iqs + j/qr + 0],
                y[iybs + iqs + j/qr + y_offset]
            });
#else
            tmp += v.x * y[iybs + iqs + j/qr + 0];
            tmp += v.y * y[iybs + iqs + j/qr + y_offset];
#endif // GGML_CUDA_F16
        }
    }

    // sum up partial sums and write back result
#pragma unroll
    for (int mask = 16; mask &gt; 0; mask &gt;&gt;= 1) {
        tmp += __shfl_xor_sync(0xffffffff, tmp, mask, 32);
    }

    if (tid == 0) {
#ifdef GGML_CUDA_F16
        dst[row] = tmp.x + tmp.y;
#else
        dst[row] = tmp;
#endif // GGML_CUDA_F16
    }
}
</code></pre>
<h2 id="ggml_cuda_mul_mat_sparse-æœ‰ä»€ä¹ˆä¸åŒ">ggml_cuda_mul_mat_sparse æœ‰ä»€ä¹ˆä¸åŒ</h2>
<p>sparseè¿™è¾¹ï¼Œåé‡åŒ–ç‰ˆæœ¬è°ƒç”¨çš„æ˜¯<code>ggml_cuda_op_mul_mat_vec_sparse_dequantized</code>ã€‚</p>
<pre><code class="language-c++">static void ggml_cuda_mul_mat_sparse(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {
    GGML_ASSERT(dst-&gt;src[2] != NULL &amp;&amp; &quot;dst-&gt;src[2] must be present for sparse matrix multiplication&quot;);
    if (src1-&gt;ne[1] == 1 &amp;&amp; src0-&gt;ne[0] % GGML_CUDA_DMMV_X == 0) {
        switch(src0-&gt;type) {
            case GGML_TYPE_F16:
                ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_mul_mat_vec_sparse_dequantized, false);
                break;
            case GGML_TYPE_Q4_0:
                ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_mul_mat_vec_sparse_q, true);
                break;
            default:
                GGML_ASSERT(false &amp;&amp; &quot;unsupported type for sparse matrix multiplication&quot;);
        }
    } else {
        ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_mul_mat_batch_sparse, false);
    }
}
</code></pre>
<p><code>ggml_cuda_op_mul_mat_vec_sparse_dequantized</code>é‡Œé¢ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°è¿™é‡Œä»<code>dst-&gt;src</code>é‡Œå–äº†<code>sparse_idx</code>å’Œ<code>row_lookup</code>è¿™ä¹ˆä¸¤ä¸ªä¸œè¥¿ã€‚</p>
<pre><code class="language-c++">inline void ggml_cuda_op_mul_mat_vec_sparse_dequantized(
    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, const char * src0_dd_i, const float * src1_ddf_i,
    const char * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const int64_t src1_ncols,
    const int64_t src1_padded_row_size, const cudaStream_t &amp; stream) {

    const int64_t ne00 = src0-&gt;ne[0];
    const int64_t ne10 = src1-&gt;ne[1];
    const int64_t row_diff = row_high - row_low;

    float * sparse_idx = static_cast&lt;float *&gt;(ggml_cuda_get_tensor_data(dst-&gt;src[2]));
    int32_t * row_lookup = dst-&gt;src[3] != NULL ? static_cast&lt;int32_t *&gt;(ggml_cuda_get_tensor_data(dst-&gt;src[3])) : NULL;

    // on some GPUs it is faster to convert src1 to half and to use half precision intrinsics
#ifdef GGML_CUDA_F16
    size_t ash;
    dfloat * src1_dfloat = nullptr; // dfloat == half

    bool src1_convert_f16 = src0-&gt;type == GGML_TYPE_Q4_0 || src0-&gt;type == GGML_TYPE_Q4_1 ||
        src0-&gt;type == GGML_TYPE_Q5_0 || src0-&gt;type == GGML_TYPE_Q5_1 ||
        src0-&gt;type == GGML_TYPE_Q8_0 || src0-&gt;type == GGML_TYPE_F16;

    if (src1_convert_f16) {
        src1_dfloat = (half *) ggml_cuda_pool_malloc(ne00*sizeof(half), &amp;ash);
        ggml_cpy_f32_f16_cuda((const char *) src1_ddf_i, (char *) src1_dfloat, ne00,
                                ne00, 1, sizeof(float), 0, 0,
                                ne00, 1, sizeof(half),  0, 0, stream);
    }
#else
    const dfloat * src1_dfloat = (const dfloat *) src1_ddf_i; // dfloat == float, no conversion
#endif // GGML_CUDA_F16

    cudaMemsetAsync((void *)dst_dd_i, 0, ggml_nbytes(dst), stream);
    switch (src0-&gt;type) {
        case GGML_TYPE_F16:
            convert_mul_mat_vec_f16_cuda_sparse(src0_dd_i, src1_dfloat, dst_dd_i, ne00, row_diff, stream, row_lookup, sparse_idx);
            break;
        default:
            GGML_ASSERT(false &amp;&amp; &quot;Unsupported type&quot;);
            break;
    }

    (void) src1;
    (void) dst;
    (void) src1_ddf_i;
    (void) src1_ncols;
    (void) src1_padded_row_size;
}
</code></pre>
<p>è€Œå®é™…ä¸Šï¼Œè¿™é‡Œçš„<code>row_lookup</code>å°±æ˜¯<code>gpu_bucket</code>ï¼Œè®°å½•çš„æ˜¯æœ‰å“ªäº›neuronsåœ¨GPUä¸Šï¼Œå³è¿™äº›neuronsåœ¨åŸçŸ©é˜µä¸­çš„indexã€‚è€Œ<code>sparse_idx</code>åˆ™æ˜¯è¿è¡Œæ—¶é€šè¿‡predictoré¢„æµ‹ä¼šè¢«æ¿€æ´»çš„neuronsçš„indexã€‚</p>
<pre><code class="language-c++">struct ggml_tensor * ggml_mul_mat_idx_upscale(
        struct ggml_context * ctx,
        struct ggml_tensor  * a,
        struct ggml_tensor  * b,
        struct ggml_tensor  * sparse_idx,
        struct ggml_tensor  * gpu_bucket,
                      int64_t result_ne0) {
    bool is_node = false;

    if (a-&gt;grad || b-&gt;grad) {
        is_node = true;
    }

    const int64_t ne[4] = { result_ne0, b-&gt;ne[1], b-&gt;ne[2], b-&gt;ne[3] };
    struct ggml_tensor * result = ggml_new_tensor(ctx, GGML_TYPE_F32, MAX(a-&gt;n_dims, b-&gt;n_dims), ne);

    result-&gt;op   = GGML_OP_MUL_MAT_SPARSE;
    result-&gt;grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;
    result-&gt;src[0] = a;
    result-&gt;src[1] = b;
    result-&gt;src[2] = sparse_idx;
    result-&gt;src[3] = gpu_bucket;

    return result;
}
</code></pre>
<p>ç„¶åï¼Œ<code>row_lookup</code>å’Œ<code>sparse_idx</code>ä¼šè¢«ä½œä¸ºå¤šå‡ºæ¥çš„ä¸¤ä¸ªå‚æ•°<code>lst</code>å’Œ<code>idx</code>ä¼ é€’åˆ°<code>dequantize_mul_mat_vec_sparse</code>ã€‚</p>
<pre><code class="language-c++">static void convert_mul_mat_vec_f16_cuda_sparse(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, cudaStream_t stream, int *lst, float *idx) {
    GGML_ASSERT(ncols % GGML_CUDA_DMMV_X == 0);
    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;
    const dim3 block_nums(1, block_num_y, 1);
    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);

    dequantize_mul_mat_vec_sparse&lt;1, 1, convert_f16&gt;
        &lt;&lt;&lt;block_nums, block_dims, 0, stream&gt;&gt;&gt;(vx, y, dst, ncols, nrows, lst, idx);
 
}
</code></pre>
<p>åˆ°äº†kernelè¿™è¾¹ï¼Œå…¶å®æ”¹åŠ¨å°±å¾ˆç®€å•äº†ã€‚<br>
å…ˆä½¿ç”¨<code>gpu_row</code>å»<code>lst</code>ï¼ˆä¹Ÿå°±æ˜¯<code>row_lookup</code>æˆ–è€…è¯´<code>gpu_bucket</code>ï¼‰æŸ¥è¯¢ä¸€ä¸‹å…¶å¯¹åº”çš„æ˜¯åŸçŸ©é˜µçš„å“ªä¸ª<code>neuron</code>ä¹Ÿå°±æ˜¯å“ªä¸€è¡Œ<code>row</code>ï¼Œç„¶åå†å»<code>idx</code>ï¼ˆä¹Ÿå°±æ˜¯é€šè¿‡predictorå¾—åˆ°çš„<code>sparse_idx</code>ï¼‰çœ‹ä¸€ä¸‹å…¶é¢„æµ‹å€¼æœ‰æ²¡æœ‰è¾¾åˆ°<code>dex_sparse_threshold</code>é˜ˆå€¼ã€‚è‹¥æ²¡æœ‰ï¼Œåˆ™ç›´æ¥<code>return</code>ä¹Ÿå°±æ˜¯è·³è¿‡å…¶è®¡ç®—ã€‚<br>
å°±æ˜¯è¿™ä¹ˆç®€å•ï¼Œå…¶ä»–éƒ¨åˆ†æ²¡æœ‰å¤šå¤§ä¿®æ”¹ã€‚</p>
<pre><code class="language-c++">template &lt;int qk, int qr, dequantize_kernel_t dequantize_kernel&gt;
static __global__ void dequantize_mul_mat_vec_sparse(const void * __restrict__ vx, const dfloat * __restrict__ y, float * __restrict__ dst, const int ncols, const int nrows, int * lst, float * idx) {
    // qk = quantized weights per x block
    // qr = number of quantized weights per data value in x block
    const int gpu_row = blockIdx.y*blockDim.y + threadIdx.y;

    if (gpu_row &gt;= nrows) {
        return;
    }

    int row = lst ? lst[gpu_row] : gpu_row;
    if (idx[row] &lt; dev_sparse_threshold) {
        return;
    }

    const int tid = threadIdx.x;

    const int iter_stride = 2*GGML_CUDA_DMMV_X;
    const int vals_per_iter = iter_stride / WARP_SIZE; // num quantized vals per thread and i iter
    const int y_offset = qr == 1 ? 1 : qk/2;

// partial sum for each thread
#ifdef GGML_CUDA_F16
    half2 tmp = {0.0f, 0.0f}; // two sums for f16 to take advantage of half2 intrinsics
#else
    float tmp = 0.0f;
#endif // GGML_CUDA_F16

    for (int i = 0; i &lt; ncols; i += iter_stride) {
        const int col = i + vals_per_iter*tid;
        const int ib = (gpu_row*ncols + col)/qk; // x block index
        const int iqs = (col%qk)/qr; // x quant index
        const int iybs = col - col%qk; // y block start index

// processing &gt;2 values per i iter is faster for fast GPUs
#pragma unroll
        for (int j = 0; j &lt; vals_per_iter; j += 2) {
            // process 2 vals per j iter

            // dequantize
            // for qr = 2 the iqs needs to increase by 1 per j iter because 2 weights per data val
            dfloat2 v;
            dequantize_kernel(vx, ib, iqs + j/qr, v);

            // matrix multiplication
            // for qr = 2 the y index needs to increase by 1 per j iter because of y_offset = qk/2
#ifdef GGML_CUDA_F16
            tmp += __hmul2(v, {
                y[iybs + iqs + j/qr + 0],
                y[iybs + iqs + j/qr + y_offset]
            });
#else
            tmp += v.x * y[iybs + iqs + j/qr + 0];
            tmp += v.y * y[iybs + iqs + j/qr + y_offset];
#endif // GGML_CUDA_F16
        }
    }

    // sum up partial sums and write back result
#pragma unroll
    for (int mask = 16; mask &gt; 0; mask &gt;&gt;= 1) {
        tmp += __shfl_xor_sync(0xffffffff, tmp, mask, 32);
    }

    if (tid == 0) {
#ifdef GGML_CUDA_F16
        dst[row] = tmp.x + tmp.y;
#else
        dst[row] = tmp;
#endif // GGML_CUDA_F16
    }
}
</code></pre>
<h2 id="æ€»ç»“">æ€»ç»“</h2>
<p>å¯¹äºåŸç‰ˆçš„ç®—å­ï¼Œè¿™é‡Œä»‹ç»çš„æ¯”è¾ƒç®€å•ã€‚æˆ‘ä»¬ä¸»è¦å…³æ³¨PowerInferåœ¨è¿™é‡Œåšäº†ä»€ä¹ˆä¿®æ”¹ã€‚<br>
å…¶å®ç›¸æ¯”åŸå…ˆçš„å®ç°ï¼ŒPowerInferè¿™è¾¹çš„<code>sparse</code>ç‰ˆæœ¬æ‰€åšçš„ä¿®æ”¹å°±å¾ˆç®€å•ã€‚å°±æ˜¯æ£€æŸ¥ä¸€ä¸‹å…¶predictorçš„é¢„æµ‹å€¼æœ‰æ²¡æœ‰è¾¾åˆ°æŒ‡å®šçš„é˜ˆå€¼ï¼Œè‹¥æ²¡æœ‰åˆ™ç›´æ¥<code>return</code>è·³è¿‡è®¡ç®—å³å¯ã€‚<br>
åˆ°è¿™é‡Œï¼Œä»ä¸Šåˆ°ä¸‹çš„ä¸€ä¸ªç®€å•è¿è¡Œæµç¨‹å°±è¢«èµ°é€šäº†ï¼Œå¯¹PowerInferçš„æºç è§£æä¹Ÿåˆæ­¥ç»“æŸäº†ã€‚<br>
å¯èƒ½è¿˜æœ‰ä¸‹ä¸€ç¯‡è§£æï¼Œä¹Ÿå¯èƒ½æ²¡æœ‰äº†ã€‚</p>
<h2 id="æ”¯æŒ-ï¸">æ”¯æŒ â˜•ï¸</h2>
<p>å¦‚æœå‘ç°å†…å®¹æœ‰çº°æ¼æˆ–é”™è¯¯ï¼Œå¯ä»¥é€šè¿‡é‚®ç®±hangyu.yuan@qq.comè”ç³»æˆ‘æˆ–ç›´æ¥åœ¨ä¸‹æ–¹è¯„è®ºå‘Šè¯‰æˆ‘ï¼Œè°¢è°¢ã€‚<br>
æˆ‘çš„<a href="https://github.com/Yuan-Allen">GitHubä¸»é¡µ</a>ã€‚</p>

              </div>
              <div class="toc-container">
                <ul class="markdownIt-TOC">
<li>
<ul>
<li><a href="#%E7%9B%B8%E5%85%B3">ç›¸å…³</a></li>
<li><a href="#ggml_compute_forward_mul_mat">ggml_compute_forward_mul_mat</a></li>
<li><a href="#ggml_cuda_mul_mat">ggml_cuda_mul_mat</a>
<ul>
<li><a href="#ggml_cuda_op_dequantize_mul_mat_vec">ggml_cuda_op_dequantize_mul_mat_vec</a></li>
</ul>
</li>
<li><a href="#ggml_cuda_mul_mat_sparse-%E6%9C%89%E4%BB%80%E4%B9%88%E4%B8%8D%E5%90%8C">ggml_cuda_mul_mat_sparse æœ‰ä»€ä¹ˆä¸åŒ</a></li>
<li><a href="#%E6%80%BB%E7%BB%93">æ€»ç»“</a></li>
<li><a href="#%E6%94%AF%E6%8C%81-%EF%B8%8F">æ”¯æŒ â˜•ï¸</a></li>
</ul>
</li>
</ul>

              </div>
            </div>
          </article>
        </div>

        
          <div class="next-post">
            <div class="next">ä¸‹ä¸€ç¯‡</div>
            <a href="https://alleny.xyz/post/powerinfer-source-analysis-2/">
              <h3 class="post-title">
                PowerInferæºç è§£æï¼ˆäºŒï¼‰ï¼šè®¡ç®—å›¾æ„å»ºä¸ç®—å­è°ƒç”¨
              </h3>
            </a>
          </div>
        

        
          

          
            <link rel="stylesheet" href="https://unpkg.com/disqusjs@1.1/dist/disqusjs.css">
<script src="https://unpkg.com/disqusjs@1.1/dist/disqus.js"></script>

<div id="disqus_thread"></div>

<script>

var options = {
  shortname: 'alleny-blog',
  apikey: '2kPJh2tQ0rWB7n0QOhvb9TLbm946tHMWCixW2qGF9j8tfMBgZoNPfvLpDoxZTZpD',
}
if ('https://disqus.hangyu-yuan.workers.dev/api/') {
  options.api = 'https://disqus.hangyu-yuan.workers.dev/api/'
}
var dsqjs = new DisqusJS(options)

</script>

          
        

        <div class="site-footer">
  Copyright Â© 2023-2025 Allen Yuan. All rights reserved.
  <a class="rss" href="https://alleny.xyz/atom.xml" target="_blank">
    <i class="ri-rss-line"></i> RSS
  </a>
</div>

      </div>
    </div>

    <script>
      hljs.initHighlightingOnLoad()

      let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

      // This should probably be throttled.
      // Especially because it triggers during smooth scrolling.
      // https://lodash.com/docs/4.17.10#throttle
      // You could do like...
      // window.addEventListener("scroll", () => {
      //    _.throttle(doThatStuff, 100);
      // });
      // Only not doing it here to keep this Pen dependency-free.

      window.addEventListener("scroll", event => {
        let fromTop = window.scrollY;

        mainNavLinks.forEach((link, index) => {
          let section = document.getElementById(decodeURI(link.hash).substring(1));
          let nextSection = null
          if (mainNavLinks[index + 1]) {
            nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
          }
          if (section.offsetTop <= fromTop) {
            if (nextSection) {
              if (nextSection.offsetTop > fromTop) {
                link.classList.add("current");
              } else {
                link.classList.remove("current");    
              }
            } else {
              link.classList.add("current");
            }
          } else {
            link.classList.remove("current");
          }
        });
      });

    </script>
  </body>
</html>
